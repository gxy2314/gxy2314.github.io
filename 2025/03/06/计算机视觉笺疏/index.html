<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="2314">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2025/03/06/计算机视觉笺疏/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="计算机视觉笺疏">
<meta property="og:url" content="http://example.com/2025/03/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/redefine-og.webp">
<meta property="article:published_time" content="2025-03-05T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-01T13:57:55.275Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/%E6%91%86%E7%83%82%E7%8C%AB.jpg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/%E6%91%86%E7%83%82%E7%8C%AB.jpg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/%E6%91%86%E7%83%82%E7%8C%AB.jpg">
    <!--- Page Info-->
    
    <title>
        
            计算机视觉笺疏 | 2314的个人空间
        
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"example.com","root":"/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":2314,"subtitle":{"text":[],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"style":"default","links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.2","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.3.0"></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                2314的个人空间
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">9</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">6</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">5</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">计算机视觉笺疏</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/%E6%91%86%E7%83%82%E7%8C%AB.jpg">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">2314</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv1</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-03-06</span>
        <span class="mobile">2025-03-06</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-07-01 21:57:55</span>
            <span class="mobile">2025-07-01 21:57:55</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<h2 id="在计算机视觉看数学"><a href="#在计算机视觉看数学" class="headerlink" title="在计算机视觉看数学"></a>在计算机视觉看数学</h2><h4 id="外积"><a href="#外积" class="headerlink" title="外积"></a>外积</h4><p> <strong>外积 (Outer Product)</strong></p>
<p>​    <strong>操作对象：</strong> 两个向量（一个列向量和一个行向量）。</p>
<p>​    <strong>结果：</strong> 一个矩阵。</p>
<p>​	<strong>用途：</strong> 将两个低维向量“扩展”成一个高维矩阵。在图像处理中，它用于<strong>构造</strong>可分离的二维卷积核，即把一个一维的平滑核和一个一维的差分核组合成一个二维的边缘检测核。</p>
<p>​	<strong>数学表示：</strong> u⊗vT 或 uvT</p>
<p>​	<strong>核心思想：</strong> 元素级别的乘积，每个元素都是两个输入向量中对应元素的乘积。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250609151648989.png"
                      alt="image-20250609151648989"
                ></p>
<p>​	$tan^{−1}$&#x3D;$arctan$</p>
<h4 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250610151727902.png"
                      alt="image-20250610151727902"
                ></p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><blockquote>
<p>PS：下面的内容中我们可能会将滤波器、卷积核、模板等当作相同的概念，不必过于计较；</p>
</blockquote>
<h3 id="1-1-卷积核"><a href="#1-1-卷积核" class="headerlink" title="1.1 卷积核"></a>1.1 卷积核</h3><p>图像噪声点简单来说就是该点的像素值与该点周围的像素值差异过大，去除噪声点最直接的想法就是对噪声点和周围点进行加权平均，这就引出了卷积核的概念</p>
<blockquote>
<p>卷积核：存储权值的模板</p>
</blockquote>
<p>卷积核也称为滤波核，当我们使用卷积核对图像进行加权平均时我们称这样的卷积核为平均核</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304094457444.png"
                      alt="3*3大小的平均核"
                ></p>
<p>使用卷积核对图像进行卷积，在卷积之前一定要对其进行翻转（不翻转会导致相关），定义f是图像，g是卷积核，对图像进行卷积操作的表达式为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304094743049.png"
                     
                ></p>
<p>式子中，(m,n)表示正在进行卷积的图像f上的点坐标，(k,l)表示卷积核g上的相对坐标</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095032190.png"
                     
                ></p>
<blockquote>
<p>PS：要保证卷积图像和原图一样大，则需要提前对原图进行填充，常用的方法是周围补0、边缘填充、镜像填充…无论如何，填充的结果都是为了保证输入和输出大小固定</p>
</blockquote>
<h3 id="1-2-卷积的特性"><a href="#1-2-卷积的特性" class="headerlink" title="1.2 卷积的特性"></a>1.2 卷积的特性</h3><p>我们现在可以将卷积操作表示为数学公式filter(f)，卷积操作主要有以下特性</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095346897.png"
                     
                ></p>
<h3 id="1-3-卷积的功能"><a href="#1-3-卷积的功能" class="headerlink" title="1.3 卷积的功能"></a>1.3 卷积的功能</h3><p>前面我们通过平均核对图像进行平滑降噪引入了卷积核的概念，事实上不同的卷积核作用也不同</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095719013.png"
                      alt="原图核"
                ></p>
<p>​            </p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095735771.png"
                      alt="左移核"
                ></p>
<p>​               </p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095754127.png"
                      alt="平滑核"
                ></p>
<p>​                </p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304095821337.png"
                      alt="锐化核"
                ></p>
<p>​                  锐化核</p>
<h4 id="1-3-1-高斯平滑核"><a href="#1-3-1-高斯平滑核" class="headerlink" title="1.3.1 高斯平滑核"></a>1.3.1 高斯平滑核</h4><p>使用平滑核进行图像处理会出现振铃效果 – 因为模板的每个点的值一样，卷积过程中引入了一些不属于原图的信息</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304100121611.png"
                     
                ></p>
<p>我们希望模板的效果是这样，中心的点权值大，边缘的权值小（边缘对中心的影响太大肯定是不好的）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304100204523.png"
                     
                ></p>
<p>如何得到上述模板呢？这里引入二维高斯函数，我们称由高斯函数产生的模板为高斯核（其中(x,y)是模板的位置坐标，方差和模板大小由人为规定）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304100422403.png"
                      alt="二维高斯函数"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304100558725.png"
                      alt="高斯核"
                ></p>
<blockquote>
<p>PS：我们希望模板所有权重值求和为1，如果不是1会导致最终原图的颜色值被衰减 - 在使用高斯函数计算之后进行一个归一化处理可以保证最终的求和为1</p>
</blockquote>
<p>因为方差和模板大小都由人为规定，所以其大小也对平滑操作有不同的影响：</p>
<ul>
<li>模板大小不变改变高斯核的方差 – 方差越小表示数据越集中，表示滤波后中心（即自身）占的比重大，即平滑的效果没那么强</li>
<li>同理，固定方差调整模板的大小 – 因为涉及归一化处理，模板越小则归一化得到的中心值越大，导致滤波后被平滑的效果没那么强</li>
</ul>
<p>总结：</p>
<ul>
<li>无论是方差还是模板大小，越大导致图像越模糊，越小导致图像越清晰</li>
<li>关于窗宽和方差的经验 – 若给出方差的值，则窗宽设置为1+2*3方差，这样的窗宽好处是高斯分布几乎包含了所有值（高斯分部图像的特征），归一化的影响微乎其微</li>
</ul>
<p>高斯核本身具备一些特性：</p>
<ul>
<li>高斯核可以去除高频信号，保留低频信号</li>
<li>高斯卷积自身是另一个高斯 – 即连续使用两次方差为1的高斯核等同于使用一次方差为根号2的效果（勾股弦定理计算得到）</li>
<li>高斯核还能够分解为x方向和y方向的高斯核，连续使用两次这样的高斯核与使用一次高斯核的效果完全相同</li>
</ul>
<p>第二点和第三点特性都表达了这样一个事情 – 一个大的高斯核可以拆分为两个小的高斯核，这意味着拆分后会大大减少图像运算的时间复杂度，这也是对图像进行卷积计算中常用的一种优化方式</p>
<h4 id="锐化核"><a href="#锐化核" class="headerlink" title="锐化核"></a>锐化核</h4><p>简单理解锐化核：</p>
<ul>
<li>原图-平滑图&#x3D;边缘图</li>
<li>原图+边缘图&#x3D;锐化图</li>
<li>故 2脉冲核-平滑核&#x3D;锐化核（分配律）</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304102604511.png"
                      alt="锐化核的推导过程"
                ></p>
<p>​																		锐化核的推导过程</p>
<p>其中α是锐化因子，表示对图像进行锐化处理的程度，e是脉冲模板，g是平滑模板</p>
<h3 id="1-4-噪声处理"><a href="#1-4-噪声处理" class="headerlink" title="1.4 噪声处理"></a>1.4 噪声处理</h3><p>前面介绍了可以使用卷积核来去除噪声，下面详细介绍一下对不同的噪声具体如何选择卷积核进行处理；</p>
<p>噪声一般分为三种，椒盐噪声（黑白），脉冲噪声（全白点），高斯噪声（每个点叠加符合独立正态分布的噪声变量产生）；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304101610881.png"
                      alt="不同种类的噪声"
                ><br>                                     不同种类的噪声</p>
<p>高斯核主要用于去除高斯噪声（对于方差越小的高斯噪声去噪效果越好）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304101915781.png"
                     
                >当噪声方差较小的时候使用方差较小的高斯核就可以滤波，反之就需要方差较大的模板才行</p>
<ul>
<li>高斯滤波器的问题：去除噪声的同时边缘也被平滑了，且高斯模板的方差越大平滑效果越明显</li>
</ul>
<p>对于椒盐噪声来说使用高斯模板的效果并不好，这里需要引入新的模板 – 中值滤波器</p>
<p>中值滤波器的核没有任何权值 – 过滤方式不是加权求和，而是选取模板中的中值作为最后的值</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304102131279.png"
                     
                ></p>
<p>为什么使用中值滤波器去除椒盐噪声能够取得较好的效果呢？ – 因为使用中值滤波器不会给图像产生任何新的像素值，而如果使用平均值会导致图像出现从未有过的新的像素点</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304102301775.png"
                     
                ></p>
<ul>
<li>实际上，中值滤波对椒盐噪声和白噪声都非常有效</li>
<li>中值滤波的模板大小过大会也导致图像模糊（类似磨皮过头）</li>
</ul>
<h2 id="2-边缘"><a href="#2-边缘" class="headerlink" title="2.边缘"></a>2.边缘</h2><p>参考文章：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107239832?spm=1001.2014.3001.5502" >(8条消息) 边缘检测(Edge Detection)_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<hr>
<blockquote>
<p>边缘的作用：图像的大部分语义信息和形状信息都可以编码在边缘上，通过边缘图可以了解图像的信息，即使用边可理解表达图像</p>
</blockquote>
<p>在传统的计算机视觉和图像处理中，当我们谈论“边”（edge）时，通常指的是图像<strong>亮度或颜色值</strong>发生<strong>急剧变化</strong>的区域。</p>
<p>图像中的“边”是多种场景属性在成像平面上的投影。除了深度不连续性，它还可以反映：</p>
<ul>
<li><strong>表面方向不连续（surface orientation discontinuity）：</strong> 例如，一个立方体的两个相邻面相交形成的棱。</li>
<li><strong>材料属性变化（material property change）：</strong> 例如，木板上的油漆痕迹。</li>
<li><strong>光照变化（illumination variation）：</strong> 例如，物体投射的阴影。</li>
</ul>
<p>不同的边的类型对不同问题的理解影响不同</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304102831695.png"
                      alt="边的分类"
                ></p>
<p>深度不连续性是三维几何属性，而非直接的二维图像属性</p>
<h3 id="2-1-边缘提取"><a href="#2-1-边缘提取" class="headerlink" title="2.1 边缘提取"></a>2.1 边缘提取</h3><blockquote>
<p>边缘的特征：边缘是图像强度函数中快速变化的地方</p>
</blockquote>
<p>无论是什么类型的边，都需要先提取边缘；</p>
<p>边缘和图像其他部分的区别主要在于边缘部分的信号是突变的，那么数学中应该如何找信号突变？ – 这转化为导数操作，即对信号求导，找到导数的极值点</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103007225.png"
                     
                ></p>
<p>在二维空间中的偏导数计算公式如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103101964.png"
                      alt="导数的数学公式"
                ></p>
<p>显然上述公式不容易计算，计算机视觉中对上述式子进行改进得到近似导数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103142623.png"
                     
                ></p>
<p>针对上述公式，边缘提取的任务恰好可以用卷积来计算 – 对x方向的偏导数和y方向的偏导数分别用x方向的卷积核((-1,1)表示自身为负，右边为正)和y方向的卷积核表示</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103305714.png"
                     
                ></p>
<p>​          $Output(x,y)&#x3D;Image(x+1,y)−Image(x,y)$</p>
<h3 id="2-2-有限差分滤波器"><a href="#2-2-有限差分滤波器" class="headerlink" title="2.2 有限差分滤波器"></a>2.2 有限差分滤波器</h3><p>事实上除了最简单的x方向和y方向的卷积核，还有如下不同算子定义的模板</p>
<h4 id="2-2-1-Prewitt算子"><a href="#2-2-1-Prewitt算子" class="headerlink" title="2.2.1 Prewitt算子"></a>2.2.1 Prewitt算子</h4><p>倾向于检测出<strong>更粗、更连贯的边缘</strong></p>
<p>$Mx $(或 $Px$)：用于检测图像在水平方向（x 方向）的梯度。</p>
<p>​        它计算图像像素在其右侧和左侧邻域的加权差。中间一列为0，表示不考虑中心像素所在列对水平梯度的直接贡献。上下两行的-1和1则考虑了__垂直方向__上对水平边缘的平滑。(相当于在<strong>垂直方向（y方向）上对边缘信息进行了加权平均</strong>。)</p>
<p>$My$ (或 $Py$)：用于检测图像在垂直方向（y 方向）的梯度。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310224854849.png"
                     
                ></p>
<p>对于单个区域的噪声点不太敏感，中间为0表示用左边和右边的差异来衡量本身是否是梯度，同时利用多个像素相减可以对单个噪声点的影响进行平均；</p>
<h4 id="2-2-2-Sobel算子"><a href="#2-2-2-Sobel算子" class="headerlink" title="2.2.2 Sobel算子"></a>2.2.2 Sobel算子</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310225011122.png"
                     
                ></p>
<p>这个核是可分离的，先高斯平滑再边缘提取，该算子对噪声的敏感程度更低；</p>
<p><strong>为什么说是可分离</strong>			<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250609150940008.png"
                      alt="image-20250609150940008"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250609151106114.png"
                      alt="image-20250609151106114"
                ></p>
<p>Sobel 算子在 Prewitt 算子的基础上增加了权重的概念，认为相邻点的距离远近对当前像素点的影响是不同的，距离越近的像素点对应当前像素的影响越大，从而实现图像锐化并突出边缘轮廓；</p>
<h4 id="2-2-3-Roberts算子"><a href="#2-2-3-Roberts算子" class="headerlink" title="2.2.3 Roberts算子"></a>2.2.3 Roberts算子</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310225041751.png"
                     
                ></p>
<p>检测模板检测的信号和模板垂直，Mx检测135°的信号，My检测45°的信号；</p>
<h3 id="2-3-图像梯度"><a href="#2-3-图像梯度" class="headerlink" title="2.3 图像梯度"></a>2.3 图像梯度</h3><p>在高数中我们知道梯度表示方向导数最大的向量，在图像中梯度指向强度增长最快（信号变化最大）的方向</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103637279.png"
                     
                ></p>
<p>梯度的方向与边缘垂直，梯度的模值代表了是边缘的可能性大小</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103542709.png"
                      alt="梯度的方向"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230304103558036.png"
                      alt="梯度的模值"
                ></p>
<h3 id="2-4-高斯偏导核"><a href="#2-4-高斯偏导核" class="headerlink" title="2.4 高斯偏导核"></a>2.4 高斯偏导核</h3><p><strong>在有噪声的图像中直接计算梯度会产生非常嘈杂的结果，因此需要先进行平滑处理，而高斯偏导核将平滑和求导结合在一个操作中，提高了效率和鲁棒性。</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310225648851.png"
                     
                ></p>
<p>如果对上述图像直接用边缘提取会得到下面的图像（因为每个点周围的信号浮动都较大），我们无法得到边缘的信息</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310225703475.png"
                     
                ></p>
<p>出现上述情况的原因是原始信号有噪声，因此在对原始信号求偏导之前需要使用高斯平滑核进行平滑去噪，如果用g表示高斯平滑核，则需要对图像进行两次卷积d(f*g)&#x2F;dx，能否转换为只对图像进行一次处理？</p>
<p><strong>为什么会有两次卷积</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250609180421786.png"
                      alt="image-20250609180421786" style="zoom:80%;" 
                >

<p>利用卷积的运算性质可以交换得到f*(dg&#x2F;dx)，其中的dg&#x2F;dx我们称为高斯偏导核</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250609180511668.png"
                      alt="image-20250609180511668" style="zoom:80%;" 
                >





<hr>
<blockquote>
<p>Q：为什么使用高斯偏导核更快？</p>
</blockquote>
<p>A：dg&#x2F;dx是高斯偏导模板，相比于f作卷积操作快得多，用算出来的高斯偏导模板与图像进行卷积，此时图像f就只会被操作一次；</p>
<hr>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310230313848.png"
                      alt="高斯偏导核_三维"
                ></p>
<p>高斯偏导核是不可分离的，因为高斯g求偏导后的结果会多出一项（与高斯平滑核的区别之一）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310230429298.png"
                      alt="高斯偏导核_二维"
                ></p>
<p>二维图像中，白色为正，黑色为负，值越大；颜色越深，卷积核的数值变化与其衡量的信号垂直；</p>
<table>
<thead>
<tr>
<th>高斯平滑核</th>
<th>高斯卷积核&#x2F;偏导核</th>
</tr>
</thead>
<tbody><tr>
<td>目标：去除高频噪声</td>
<td>目标：边缘检测，提取边缘信息</td>
</tr>
<tr>
<td>高斯平滑核没有负数</td>
<td>高斯偏导核可以有负数</td>
</tr>
<tr>
<td>高斯平滑核加权求和的结果为1（否则对图像信号放大或缩小）</td>
<td>高斯偏导核求和结果为0（否则对平坦的区域卷积后结果非0表示有边，矛盾）</td>
</tr>
</tbody></table>
<p>通过增大高斯偏导核的标准差(标准差是方差的算术平方根)，可以使得图像呈现不同的结果，标准差越大滤波后的细节信息丢失的越多（对于不同的任务细节信息的重要性不同） ；</p>
<h3 id="2-5-Canny边缘提取算法"><a href="#2-5-Canny边缘提取算法" class="headerlink" title="2.5 Canny边缘提取算法"></a>2.5 Canny边缘提取算法</h3><p>Canny边缘提取算法是一个经典的边缘提取算法，主要贡献是双门限和非最大化抑制思想；</p>
<blockquote>
<p>问题1：使用简单的高斯偏导核往往会提取出一些无关紧要的噪声；</p>
<p>原因：即使使用了高斯偏导核（Gaussian Derivative Kernel），仍然有可能提取出“无关紧要的噪声”，这主要是因为以下几个原因：</p>
<ol>
<li><strong>高斯核的平滑程度有限：</strong><ul>
<li>高斯核的平滑程度由其标准差 σ（sigma）决定。σ 越大，平滑程度越高，去除的噪声越多，但同时也会使图像模糊得更厉害，可能导致真正的细微边缘丢失。</li>
<li>如果图像中的噪声水平非常高，或者噪声的频率与某些边缘的频率接近，那么即使是高斯核也无法完全去除所有噪声，而不会损失有用的边缘信息。总是在平滑和保留细节之间存在一个<strong>权衡（trade-off）</strong>。</li>
</ul>
</li>
<li><strong>噪声类型不匹配：</strong><ul>
<li>高斯核对于去除<strong>高斯噪声</strong>（服从正态分布的噪声）效果最好。</li>
<li>但如果图像中存在其他类型的噪声，例如**椒盐噪声（salt-and-pepper noise）**或**脉冲噪声**，高斯核的平滑效果可能就不那么理想。这些噪声点可能会以较大的强度出现在图像中，即使经过高斯平滑，仍然可能产生足以被检测为“边缘”的局部梯度。</li>
</ul>
</li>
<li><strong>强纹理被误认为是边缘：</strong><ul>
<li>图像中的某些纹理可能非常细致且强度变化剧烈。对于高斯偏导核来说，这些纹理的局部强度变化可能足够大，从而被误认为是边缘。</li>
<li>例如，一张布料的图像，即使没有噪声，布料本身的细密纹理也会被边缘检测算法识别为大量的“边缘”，而这些边缘在某些应用场景下可能被认为是“无关紧要的噪声”。</li>
</ul>
</li>
<li><strong>亮度不均匀：</strong><ul>
<li>如果图像存在亮度不均匀（例如光照不均，有渐变），这种渐变本身也会产生梯度。虽然这种梯度不是随机噪声，但在某些应用中，如果只想提取物体轮廓，那么这种由光照变化引起的梯度可能就是“无关紧要的”。高斯偏导核也会响应这种缓变。</li>
</ul>
</li>
<li><strong>阈值选择问题：</strong><ul>
<li>边缘检测通常会伴随一个<strong>阈值（thresholding）</strong>步骤。计算出梯度幅值图像后，只有梯度幅值大于某个阈值的像素才被认为是边缘。</li>
<li>如果阈值设置得<strong>过低</strong>，即使是经过高斯平滑后残留的微弱噪声或细微纹理，也可能因为其梯度幅值超过阈值而被误认为是边缘。</li>
<li>如果阈值设置得<strong>过高</strong>，虽然可以去除大部分噪声，但可能会导致一些真实的、强度较低的边缘被漏检。</li>
</ul>
</li>
</ol>
</blockquote>
<p>解决办法：某些区域的幅值较小可能是由于噪声引起的，设置一个门限将低于该门限的边缘去除；</p>
<blockquote>
<p>问题2：边应当很细，但是往往提取出的边比较粗</p>
</blockquote>
<p>原因：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310231144914.png"
                     
                ></p>
<p><strong><code>f(t)</code> 图像：</strong></p>
<ul>
<li><p>横轴 <code>t</code> 代表图像中的位置（可以是一维信号，也可以是图像的某一扫描线）。</p>
</li>
<li><p>纵轴 <code>f(t)</code> 代表图像的强度（亮度）。</p>
</li>
<li><p>图中显示了一个信号从低强度（左侧）逐渐平缓地过渡到高强度（右侧）的过程。这种平缓的过渡就是图上红线圈出来的区域，标记为 **”Edge”**。</p>
</li>
<li><p><strong>关键点：</strong> 这个边缘不是一个理想的<strong>阶跃（step）</strong>，而是一个<strong>斜坡（ramp）</strong>。换句话说，图像亮度不是突然跳变，而是<strong>“信号慢慢增强”</strong>。</p>
</li>
<li><p>原因分析：</p>
<p> 为什么真实的图像边缘通常是斜坡而不是阶跃呢？</p>
<ul>
<li><strong>相机模糊：</strong> 相机有限的景深、镜头衍射、运动模糊等都会使图像在边缘处产生模糊，导致亮度渐变。</li>
<li><strong>光照：</strong> 真实世界的光照通常不是完美的，物体边缘可能被半影、散射等影响，形成渐变。</li>
<li><strong>物体本身：</strong> 物体表面不是绝对平整的，或者材质本身就导致光线反射的渐变。</li>
</ul>
</li>
</ul>
<p><strong><code>∇f(t)</code> 图像：</strong></p>
<ul>
<li><p>这是上面 <code>f(t)</code> 信号的导数（梯度幅值或在某个方向的梯度值）。</p>
</li>
<li><p>由于 <code>f(t)</code> 是一个平缓的斜坡，其导数 <code>∇f(t)</code> 也呈现出一个有一定宽度的<strong>“峰值”</strong>（或“谷值”）。它不是一个数学意义上的冲激函数，而是具有一定宽度。</p>
</li>
<li><p><strong><code>--Threshold</code> (虚线)：</strong> 这表示在边缘检测中，我们通常会设定一个<strong>阈值</strong>。只有当梯度幅值<strong>大于</strong>这个阈值时，我们才认为该像素是边缘点。</p>
</li>
<li><p><code>Edge</code> (红色区域)：</p>
<p> 图中标出了<strong>“门限以内（应该是指门限以上）的区域认为都是边”</strong>。</p>
<ul>
<li>由于导数曲线是一个有宽度的“驼峰”，那么<strong>所有</strong>在这个“驼峰”中，其值高于设定的阈值的点，都会被识别为边缘点。</li>
<li>这导致原本应该是单像素宽的理想边缘，在经过阈值化后，变成了由多个像素组成的<strong>“粗边”</strong>。</li>
</ul>
</li>
</ul>
<p>解决办法：非最大化抑制，找到真正的边；</p>
<h4 id="2-5-1-非最大化抑制"><a href="#2-5-1-非最大化抑制" class="headerlink" title="2.5.1 非最大化抑制"></a>2.5.1 非最大化抑制</h4><p>做法非常简单，对于粗边上的每一个像素点，与其梯度方向的前后点的梯度比较，保留值较大者</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310231303666.png"
                     
                ></p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">NMS</span>(<span class="params">Gx, Gy</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">       非极大值抑制</span></span><br><span class="line"><span class="string">       params:</span></span><br><span class="line"><span class="string">             Gx,Gy: 图像在水平,垂直方向上的梯度</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    GraMag= np.sqrt(Gx ** <span class="number">2</span> + Gy ** <span class="number">2</span>)</span><br><span class="line">    Gx[np.<span class="built_in">abs</span>(Gx) &lt;= <span class="number">1e-5</span>] = <span class="number">1e-5</span></span><br><span class="line">    temp = np.arctan2(Gy, Gx) / np.pi*<span class="number">180.0</span></span><br><span class="line">    temp[temp &lt; -<span class="number">22.5</span>] += <span class="number">180.0</span></span><br><span class="line">    <span class="comment"># arctan (-Π/2,Π/2) ,操作后(0,Π)</span></span><br><span class="line">    angle = np.zeros_like(temp, dtype=np.uint8)</span><br><span class="line">    Height, Width = temp.shape</span><br><span class="line">    <span class="comment"># 方向角度离散化为四个主要方向</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Width):</span><br><span class="line">            <span class="keyword">if</span> temp[i, j] &lt;= <span class="number">22.5</span>:</span><br><span class="line">                angle[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="number">22.5</span> &lt; temp[i, j] &lt;= <span class="number">67.5</span>:</span><br><span class="line">                angle[i, j] = <span class="number">45</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="number">67.5</span> &lt; temp[i, j] &lt;= <span class="number">112.5</span>:</span><br><span class="line">                angle[i, j] = <span class="number">90</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="number">112.5</span> &lt; temp[i, j] &lt;= <span class="number">157.5</span>:</span><br><span class="line">                angle[i, j] = <span class="number">135</span></span><br><span class="line">    Height, Width = angle.shape</span><br><span class="line">    di0, dj0, di1, dj1 = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Width):</span><br><span class="line">            <span class="keyword">if</span> angle[i, j] == <span class="number">0</span>: </span><br><span class="line">                di0, di1, dj0, dj1 = -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> angle[i, j] == <span class="number">45</span>:</span><br><span class="line">                di0, di1, dj0, dj1 = -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">                <span class="comment"># (x-1,y+1) (x+1,y-1)</span></span><br><span class="line">            <span class="keyword">elif</span> angle[i, j] == <span class="number">90</span>:</span><br><span class="line">                di0, di1, dj0, dj1 = <span class="number">0</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> angle[i, j] == <span class="number">135</span>:</span><br><span class="line">                di0, di1, dj0, dj1 = -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">            <span class="comment"># 边缘情况处理一下</span></span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 横轴无法-1</span></span><br><span class="line">                di0 = <span class="built_in">max</span>(di0, <span class="number">0</span>)</span><br><span class="line">                dj0 = <span class="built_in">max</span>(dj0, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> j == Width - <span class="number">1</span>:</span><br><span class="line">                di0 = <span class="built_in">min</span>(di0, <span class="number">0</span>)</span><br><span class="line">                dj0 = <span class="built_in">min</span>(dj0, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                di1 = <span class="built_in">max</span>(di1, <span class="number">0</span>)</span><br><span class="line">                dj1 = <span class="built_in">max</span>(dj1, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> i == Height - <span class="number">1</span>:</span><br><span class="line">                di1 = <span class="built_in">min</span>(di1, <span class="number">0</span>)</span><br><span class="line">                dj1 = <span class="built_in">min</span>(dj1, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">max</span>(<span class="built_in">max</span>(GraMag[i, j], GraMag[i + di1, j + di0]), GraMag[i + dj1, j + dj0]) != GraMag[i, j]:</span><br><span class="line">                GraMag[i, j] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> GraMag</span><br></pre></td></tr></table></figure></div>



<h4 id="2-5-2-双门限法"><a href="#2-5-2-双门限法" class="headerlink" title="2.5.2 双门限法"></a>2.5.2 双门限法</h4><p>门限设置过高会导致原本某些边会被去除，但是门限设置的过低又会导致某些假边存在（噪声边）；</p>
<p>双门限法的思想是：先用高门限将粗狂的边检测出来 – 这些边是噪声的可能性较低，接着降低门限使得较弱的边显现，只选择与粗狂边有连接的边保留</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310231526044.png"
                     
                ></p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">doubleThresholdDetection</span>(<span class="params">src, HT, LT</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">       双门限检测</span></span><br><span class="line"><span class="string">       param:</span></span><br><span class="line"><span class="string">         HT: 高阈值,高于此阈值直接判定为边界</span></span><br><span class="line"><span class="string">         LT: 低阈值,低于此阈值直接忽略,否则归为待定</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    Height, Width = src.shape</span><br><span class="line">    src[src &gt;= HT] = <span class="number">255</span></span><br><span class="line">    src[src &lt; LT] = <span class="number">0</span></span><br><span class="line">    src = np.pad(src,[[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]], <span class="string">&#x27;constant&#x27;</span>, constant_values=(<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, Height + <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, Width + <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> LT &lt;= src[i,j] &lt;= HT:</span><br><span class="line">                max_value = -np.inf  </span><br><span class="line">                <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">1</span>, <span class="number">2</span>): </span><br><span class="line">                    <span class="keyword">for</span> dj <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">1</span>, <span class="number">2</span>):  </span><br><span class="line">                        <span class="keyword">if</span> src[i + di, j + dj] &gt; max_value:</span><br><span class="line">                            <span class="comment"># 遍历自身+周围所有点，求最大值</span></span><br><span class="line">                            max_value = src[i + di, j + dj]</span><br><span class="line">                <span class="keyword">if</span> max_value &gt;= HT:</span><br><span class="line">                    src[i, j] = <span class="number">255</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    src[i, j] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> src[<span class="number">1</span>:Height+<span class="number">1</span>, <span class="number">1</span>:Width+<span class="number">1</span>]</span><br></pre></td></tr></table></figure></div>



<h4 id="2-5-3-边缘提取算法"><a href="#2-5-3-边缘提取算法" class="headerlink" title="2.5.3 边缘提取算法"></a>2.5.3 边缘提取算法</h4><p>一个完整的边缘提取算法大致步骤如下（以Canny算法举例）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310231614530.png"
                     
                ></p>
<h2 id="3-拟合"><a href="#3-拟合" class="headerlink" title="3.拟合"></a>3.拟合</h2><p>参考文章：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107354891" >(8条消息) 拟合(Fitting)_fitting拟合_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<hr>
<blockquote>
<p>问题引出：仅靠边缘提取不能对物体进行整体的描述，提取完边缘后如何使用数学模型来描述边缘？</p>
</blockquote>
<p>边缘检测只能描述桌子上有钱币，而拟合可以描述钱币在桌子的什么地方、钱币的数学方程等</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310231954854.png"
                     
                ></p>
<blockquote>
<p>拟合的难点：</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310232210177.png"
                     
                ></p>
<ol>
<li>噪声：噪声的存在使拟合的模型偏离真实的线</li>
<li>外点：在目标图形以外的线，如上图中的目标图形为“车”，左边的“栅栏”就是外点</li>
<li>目标图形部分被遮挡，使部分图形消失</li>
</ol>
<blockquote>
<p>Overview</p>
</blockquote>
<p><strong>Fitting中常用的几种方法根据问题的难度分别如下</strong>：</p>
<ol>
<li>假如我们已经知道了所有属于这条线上的像素点（噪声点也算原本属于线上的点，只是因为噪声的影响偏离了线） – 一般使用最小二乘法找出这条线</li>
<li>假如我们知道的像素点中有不属于这条线的（外点），按照外点的多少使用不同的方法 – 外点较少使用Robust fitting，外点较多使用RANSAC</li>
<li>假如存在很多线条，对于我们需要求解的线条来说都是外点 – RANSAC或Hough transform</li>
<li>甚至我们连这是否是一条线都不能确定（无法写出它的数学方程） – 使用Snake等虚拟建模</li>
</ol>
<h3 id="3-1-最小二乘法"><a href="#3-1-最小二乘法" class="headerlink" title="3.1 最小二乘法"></a>3.1 最小二乘法</h3><blockquote>
<p>最小二乘法的前提是所有的点都处于线上（都是有效点）</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310232549486.png"
                     
                ></p>
<p>目标：找到一条线，使得这些点到线沿着纵轴方向的距离最短（注意不是垂直！！！），转化为使得优化函数E最小</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310232641838.png"
                     
                ></p>
<p> E 被定义为 Y−XB 的范数（模长）的平方：</p>
<p>经过上述变化可以直接求解B，使用最小二乘法求解得到的B是实际XB&#x3D;Y方程的近似解</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250610165339694.png"
                      alt="image-20250610165339694" style="zoom:80%;" 
                >

<p>最小二乘法的问题：</p>
<ol>
<li>假如要估计的直线是垂直的则无法使用最小二乘法；</li>
<li>假如摄像头旋转过后，原本可求解的直线可能就变得无法求解；</li>
</ol>
<h3 id="3-2-权最小二乘法"><a href="#3-2-权最小二乘法" class="headerlink" title="3.2 权最小二乘法"></a>3.2 权最小二乘法</h3><p>目标：找到一条线，使得这些点到线沿着纵轴方向的距离最短，此处的距离为点到直线的垂直距离，无论怎么旋转都不会影响点和直线之间的关系</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310232928138.png"
                     
                ></p>
<p>使用上述的模型则优化函数E的表达式和待求解矩阵N的关系推导如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310233031268.png"
                     
                ></p>
<p>在函数的最低点（或最高点），函数的斜率（即导数）为 0。对于一个凸函数（本例中的 E 就是一个关于参数的二次函数，其图像是碗状的，所以是凸函数），导数为 0 的点就是其全局最小值点。</p>
<p>权最小二乘法的几何解释：找到一条直线，使得所有点在法向量方向上的投影最小</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310233113497.png"
                     
                ></p>
<p>权最小二乘法的问题：全最小二乘对外点来说不适用，拟合性很差</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310233229415.png"
                     
                ></p>
<h3 id="3-3-鲁棒性最小二乘"><a href="#3-3-鲁棒性最小二乘" class="headerlink" title="3.3 鲁棒性最小二乘"></a>3.3 鲁棒性最小二乘</h3><blockquote>
<p>假如点到直线的距离过大（外点），那么认为其贡献有上限</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310233334133.png"
                     
                ></p>
<p>使用鲁棒性最小二乘可以缓解外点带来的影响</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230310233520535.png"
                     
                ></p>
<ul>
<li>参数过小会导致每个点都没有贡献</li>
<li>参数过大会导致出现与权最小二乘相同的问题</li>
</ul>
<blockquote>
<p>鲁棒性最小二乘只能用迭代方法求解（因为不是线性优化问题）</p>
</blockquote>
<h3 id="3-4-RANSAC"><a href="#3-4-RANSAC" class="headerlink" title="3.4 RANSAC"></a>3.4 RANSAC</h3><p>RANSAC也称为随机采样一致性，适用于外点较多的情况下，使用较少的点来拟合真实的模型；</p>
<p>RANSAC算法的主要步骤如下：</p>
<ul>
<li>随机选择一个最小的集合s（对于拟合直线方程任务来说最小集合是两个点 – 两点确定一条直线）</li>
<li>拟合出一个模型（对于直线拟合来说就是利用集合中的两个点写出直线方程）</li>
<li>设置一个门限t（直线拟合任务中门限内的点我们认为是内点，门限外的点认为是外点）</li>
<li>用门限t内剩余的点给这个模型“投票”（最简单的投票方式就是有几个点投几票）</li>
<li>重复上述过程，取“得分”最高的模型，记录迭代次数N（这里的N就是得到最好模型一共进行的试验次数）<ul>
<li>最后的输出结果有两种选择，一种是输出评分最高的模型，一种是输出大于一定门限值的一组模型（可用于存在外线的拟合）</li>
<li>最后输出的模型并不是评分最高确定的直线模型，而是会将该直线和内点再次使用权最小二乘方法进行拟合</li>
</ul>
</li>
</ul>
<blockquote>
<p>RANSAC算法是一种框架，这意味着RANSAC算法不仅可用于拟合直线方程，也可以进行其他应用（如指纹匹配等）；</p>
</blockquote>
<p>RANSAC算法主要有五个参数：</p>
<ul>
<li>最小集合s：拟合模型所需的最小数据点数量。</li>
<li>门限t：用于判断一个点是否是内点（inlier）的距离阈值。如果一个点到当前模型的距离小于 t，则被认为是内点。</li>
<li>拟合准确率p：我们希望 RANSAC 算法能成功找到一个良好模型的概率。</li>
<li>外点率e</li>
<li>迭代次数N</li>
</ul>
<p>一般情况下已知s,t,p和e，我们可以直接根据下面的公式计算出迭代次数N</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314201653278.png"
                     
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250610172322068.png"
                      alt="image-20250610172322068"
                ></p>
<p>而实际任务过程中，外点率e通常是未知的（因为我们要拟合的线是什么都不知道，又怎么可能知道哪些点不属于这条线？），如何确定迭代次数N呢？</p>
<p>这里采用一种更普适的RANSAC方法 – 自适应参数提取RANSAC</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314202137408.png"
                     
                ></p>
<h3 id="3-5-Hough-transform"><a href="#3-5-Hough-transform" class="headerlink" title="3.5 Hough transform"></a>3.5 Hough transform</h3><p>基本思想：</p>
<ul>
<li>霍夫变换同样是使用图像上的所有点为直线模型投票，选择票数最高的模型进行输出；</li>
<li>噪声点不会有一致性的答案；</li>
<li>即使中间的某些点被遮挡也能拟合一个好的模型；</li>
</ul>
<p>霍夫变换首先需要引入原始图像空间和离散参数空间（将参数空间划离散化为一个个grid）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314223050529.png"
                     
                ></p>
<p>霍夫变换的原理是图像空间中的一个点对应参数空间的一条直线，那么图像空间中的两个点对应参数空间中的两条直线，参数空间中这两条直线的交点对应的参数实际就是图像空间中两点确定的直线的参数；通过在参数空间中设置一个“累加器”网格，统计每个交点的“投票”数量。得票最高的参数空间中的点，就对应着图像空间中最可能存在的直线。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314222710832.png"
                     
                ></p>
<p>当图像空间中的点足够多的时候，参数空间中会存在多条直线，这些直线的交点（理解为得分最高的grid）就是图像空间中拟合的直线模型的参数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314223202917.png"
                     
                ></p>
<p>直角坐标的缺点：</p>
<ul>
<li>对于m，b两个参数构成的参数空间本身是无范围的，这就给离散化带来困难（grid无穷）；</li>
<li>对于垂直直线m的范围为无穷，在参数空间中无法表示；</li>
</ul>
<p>引入极坐标系解决问题，其中θ∈(0°,180°)，下面是如何将图像空间中的直线上的点转换为极坐标参数空间下的直线并进行投票的算法（也是真正常用的霍夫变换）。与 y&#x3D;mx+b 类似，图像空间中的一个点 (x,y)，在极坐标参数空间 (ρ,θ) 中，也对应着一条曲线（通常是正弦曲线）。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314224211636.png"
                     
                ></p>
<p><strong>Initialize accumulator H to all zeros</strong>：</p>
<ul>
<li><p>创建一个二维数组（或矩阵）<code>H</code>，称为<strong>累加器（accumulator array）</strong>。</p>
</li>
<li><p>这个累加器的维度对应于参数空间中的 ρ 和 θ 的离散化步长。例如，如果 θ 的步长是 1∘，ρ 的步长是 1 像素，那么累加器的大小就是 (图像对角线长度) ×180。</p>
</li>
<li><p>初始化累加器中的所有值为 0。</p>
</li>
</ul>
<p>参数空间中的 θ 是<strong>从原点到直线的垂线与 X 轴正方向的夹角</strong>。</p>
<p><strong>For each edge point (x,y) in the image</strong>：</p>
<ul>
<li>遍历图像中所有的<strong>边缘点</strong>（这些点通常是预先通过边缘检测算法，如 Canny 或 Sobel 算子得到的）。</li>
<li><strong>For θ&#x3D;0 to 180</strong>：<ul>
<li>对于每一个边缘点 (x,y)，我们假设它可能是通过任何方向的直线。因此，我们遍历 θ 的所有可能值（从 0∘ 到 180∘，通常以某个步长递增）。</li>
<li><strong>ρ&#x3D;xcosθ+ysinθ</strong>：<ul>
<li>根据当前边缘点 (x,y) 和当前的 θ 值，计算出对应的 ρ 值。这相当于找到一条过点 (x,y) 的直线的垂线距离，<strong>其中该垂线与 X 轴正方向的夹角为 θ</strong>。”</li>
</ul>
</li>
<li><strong>H(θ, ρ) &#x3D; H(θ, ρ) + 1</strong>：<ul>
<li>在累加器 <code>H</code> 中，找到由计算出的 (ρ,θ) 对所对应的“bin”（离散化后的单元格）。</li>
<li>将该 bin 的值加 1。这被称为“投票”。一个边缘点会对其可能所属的每条直线（即每个 (ρ,θ) 对）进行投票。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Find the value(s) of (θ, ρ) where H(θ, ρ) is a local maximum</strong>：</p>
<ul>
<li>在所有边缘点都投票完毕后，遍历累加器 <code>H</code>。</li>
<li>找到累加器中值最高的（或局部最大值的）那些 (ρ,θ) 对。这些高值表明有许多边缘点在图像空间中共线，并且它们的投票在参数空间的同一个 (ρ,θ) bin 中汇聚。</li>
</ul>
<p><strong>The detected line in the image is given by ρ&#x3D;xcosθ+ysinθ</strong>：</p>
<ul>
<li>累加器中的局部最大值对应的 (ρ,θ) 值，就是图像中检测到的直线的参数。将这些参数代回直线方程，就可以在图像中绘制出检测到的直线了。</li>
</ul>
<h4 id="3-5-1-噪声影响"><a href="#3-5-1-噪声影响" class="headerlink" title="3.5.1 噪声影响"></a>3.5.1 噪声影响</h4><p>在霍夫变换中噪声的影响是非常大的</p>
<p>​      自身的噪声：峰值变得模糊且难以定位</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314224618115.png"
                      alt="img"
                ></p>
<p>​         外界的噪声：随机噪声会导致出现很多伪峰值</p>
<p>​		 <img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314224745399.png"
                      alt="img"
                ></p>
<p>由于图像空间中的点并非完美地共线，而是在一条“模糊的直线”上，它们在参数空间中对应的曲线不会完美地汇聚到一个尖锐的交点。相反，它们会在一个区域内形成一个<strong>“模糊的峰值”</strong>，而不是一个清晰的、孤立的亮点。这使得找到代表真实直线的精确参数变得困难。</p>
<p>解决上述问题主要有如下三种方法：</p>
<ul>
<li>选取合适大小的grid，即对参数空间进行适当的离散化；</li>
<li>投票的时候采取“软投票”策略：即不仅仅只对中心网格投票，也适当的按照一定比例给网格周围的grid投票；</li>
<li>Canny算子<ul>
<li>只采用检测出的边上的点进行投票；</li>
<li>因为在Candy算子中得到点时就知道了梯度方向，相应的边缘方向的范围就大概确认了，故可以缩小θ的范围，从而解决了噪声的影响，也简化了计算；</li>
</ul>
</li>
</ul>
<p>于是改进霍夫变换如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314225332181.png"
                     
                ></p>
<p><strong>解决上述问题主要有如下三种方法：</strong></p>
<ol>
<li><p><strong>选取合适大小的grid，即对参数空间进行适当的离散化：</strong></p>
<ul>
<li>这是霍夫变换的基本实践。参数空间 (ρ,θ) 并不是连续的，而是被划分为一个离散的网格（<code>accumulator array</code>）。</li>
<li>如果网格过粗（<code>grid</code> 太大，即 ρ 和 θ 的步长太大），会导致不同的直线参数被映射到同一个单元格，使得峰值不精确，丢失细节。</li>
<li>如果网格过细（<code>grid</code> 太小，即 ρ 和 θ 的步长太小），虽然能提高精度，但会增加计算量和存储需求，而且容易因为噪声导致真实的峰值分散到多个相邻的单元格中，反而使峰值不易被检测到。</li>
<li>因此，选择一个合适的 <code>grid</code> 大小是平衡精度和鲁棒性的关键。</li>
</ul>
</li>
<li><p><strong>投票的时候采取“软投票”策略：即不仅仅只对中心网格投票，也适当的按照一定比例给网格周围的grid投票：</strong></p>
<ul>
<li>这是一种处理“自身的噪声”（模糊峰值）的方法。</li>
<li>在标准的霍夫变换中，一个边缘点只对其计算出的精确 (ρ,θ) 单元格投票。</li>
<li>“软投票”策略（或称为<strong>加权投票</strong>）意味着，当一个边缘点投票时，它不仅给计算出的主单元格投一票，也会给其<strong>周围相邻的单元格</strong>投一些票，票数按照距离中心的远近适当加权（例如，距离越近，票数越多）。</li>
<li><strong>好处：</strong> 这样可以应对由于图像模糊或像素量化导致的参数空间中峰值的轻微分散。即使真实的峰值稍微偏离精确的单元格，它周围的单元格也能累积到足够多的票数，从而使模糊的峰值变得更容易被识别，降低了对网格离散化精度的敏感性。</li>
</ul>
</li>
<li><p><strong>Canny算子：</strong></p>
<ul>
<li><p>Canny 算子是一种优秀的边缘检测算法，它不仅能检测边缘，还能提供边缘的<strong>梯度方向</strong>。</p>
</li>
<li><p>只采用Canny算子检测出的边上的点进行投票：</p>
<ul>
<li>这是霍夫变换通常的输入。霍夫变换不是直接在原始图像上运行，而是作用于经过边缘检测后的二值图像（只包含边缘点）。Canny 算子能够提供高质量的细化边缘，减少了输入到霍夫变换中的噪声点。</li>
</ul>
</li>
<li><p>因为在Canny算子中得到点时就知道梯度方向，相应的边的方向就大概确认了，故可以缩小θ的范围，从而解决了噪声的影响，也简化了计算；</p>
<ul>
<li><p><strong>关键改进：</strong> 这点是图中给出的改进霍夫变换算法的核心。传统的霍夫变换对于每个边缘点，会遍历所有的 θ 范围（0∘ 到 180∘）。但是，Canny 算子在检测边缘的同时，会计算出每个边缘点的<strong>梯度方向</strong>。</p>
</li>
<li><p>由于我们之前讨论过，直线的<strong>法线方向</strong>（即霍夫参数空间中的 θ）与<strong>梯度方向</strong>是平行的（或角度差为 0∘ 或 180∘，取决于方向的定义），或者与<strong>边缘本身的方向</strong>垂直。</p>
</li>
<li><p>因此，对于每个边缘点 </p>
<p>(x,y)</p>
<p>，我们不需要遍历所有的 </p>
<p>θ</p>
<p> 值。我们只需要考虑其</p>
<p>梯度方向附近的少量 θ 范围</p>
<p>进行投票即可。这大大缩小了 </p>
<p>θ</p>
<p> 的搜索空间，减少了无效的投票，从而：</p>
<ul>
<li><strong>解决了噪声的影响：</strong> 减少了随机噪声点在参数空间中产生伪峰值的机会，因为每个噪声点只在一个很小的 θ 范围内投票。</li>
<li><strong>简化了计算：</strong> 大大减少了内层循环的迭代次数（从 180&#x2F;步长 减少到 1 或很少的几个值），提高了算法的效率。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="3-5-2-圆拟合"><a href="#3-5-2-圆拟合" class="headerlink" title="3.5.2 圆拟合"></a>3.5.2 圆拟合</h4><p>直线空间上的一个点如何推测圆心和半径？（即如何给参数空间投票）</p>
<p>给圆上一点和半径r，可以在参数空间对应两个点（圆心位置为圆周点加减半径，方向由梯度方向确定），圆心必定是投票数最高的地方，此时的(x,y,r)就是圆心坐标和圆半径</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230314231613633.png"
                     
                ></p>
<p><strong>梯度 ∇I(x,y)<strong>：这个梯度是在图像空间中的边缘点 (x,y) 处计算的。它表示在该点处图像强度变化最快的方向和速率。对于一个边缘点，其梯度方向通常是</strong>垂直于边缘</strong>的。</p>
<p>在实际的求解中并不知道圆周上每一点(x,y)以及半径，所以需要穷举所有的r（大于0小于图像长度），遍历圆上每一点进行投票；</p>
<h4 id="3-5-3-小结"><a href="#3-5-3-小结" class="headerlink" title="3.5.3 小结"></a>3.5.3 小结</h4><p>霍夫变化对参数空间非常敏感，如果空间维度高了霍夫变化的运算量会非常大</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324103430042.png"
                     
                ></p>
<h2 id="4-区域检测-角点"><a href="#4-区域检测-角点" class="headerlink" title="4.区域检测-角点"></a>4.区域检测-角点</h2><h3 id="4-1-特征点"><a href="#4-1-特征点" class="headerlink" title="4.1 特征点"></a>4.1 特征点</h3><p>文章参考：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107402763?spm=1001.2014.3001.5502" >(15条消息) 区域检测——Harris角点_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324154102036.png"
                     
                ></p>
<p>图像处理过程中有一种任务是图像拼接，图像拼接即实现全景拍摄的一种手段，需要分析图片结构提取其特征点；</p>
<p>实现图像拼接具体步骤是什么？提取图像特征点采用的具体算法是什么？下面将介绍</p>
<p>图像拼接主要分为如下三个步骤：</p>
<ol>
<li>提取特征点</li>
<li>匹配特征点</li>
<li>使用RANSAC方法将两张图片的对应的特征点转换的方式拟合出来，然后对图片采用相同的转换方式进行转换，最后进行拼接</li>
</ol>
<p>本节主要讲解第一步如何提取特征点，首先要知道什么样的像素点能够充当特征点，主要需要满足如下四个特性：</p>
<ul>
<li>可重复性：在一张图可以被观测到的，在其他同场景的图也可以被观测到</li>
<li>显著性：检测的特征点需要是在某一类图像中“独有的”，尽量剔除“普遍性”的点，目的是为了将不同类的图区分开</li>
<li>简洁和高效：尽可能的减少计算量，提高计算效率</li>
<li>局部性：特征计算的时候只与局部有关，与全局无关，这样两张图的同一个特征计算出来值才会接近</li>
</ul>
<blockquote>
<p>角点导数在两个及以上方向有变化的点满足上述四个条件，因此通常选择角点作为特征点；</p>
</blockquote>
<p>下面介绍如何提取图像中的角点的算法；</p>
<h3 id="4-2-特征提取"><a href="#4-2-特征提取" class="headerlink" title="4.2 特征提取"></a>4.2 特征提取</h3><p>与平坦区域的点和边上的点不同的是，角点的导数在两个及以上方向有变化，因此最基本的想法为</p>
<ol>
<li>使用一个较小的窗口在图像上沿各个方向滑动，该窗口可以包含图像的多个像素点</li>
<li>窗口移动前后不同的差异值显示了窗口内部像素点的信息</li>
<li>图像内部所在的窗口沿各个方向都没有变化（即内部的点的导数变化均为0，表现为窗口前后差异值为0）；边缘所在的窗口沿边缘方向无变化；角点所在窗口会在各个方向上都有显著的变化（即若窗口内含有角点，则该窗口无论沿任何方向移动其前后差异值都不为0）</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324155330698.png"
                     
                ></p>
<p>将上述思想描述为数学公式如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324155439402.png"
                     
                ></p>
<p>公式的意思是平移后的窗口与平移前的窗口的对应位置差的平方累加求和，E(u,v)就表示两个窗口内容的差异值的总和，其中</p>
<ul>
<li>u和v是平移量；</li>
<li>考虑到每个点对窗口影响的不同程度，因此可以乘以窗口权重，赋予不同位置的点的差值对E(u,v)的影响程度；</li>
</ul>
<p>如何直接建立u,v和E(u,v)之间的关系而不是如上面式子一样需要对应图像像素I(x,y)？这里使用泰勒展开式，在u&#x3D;0,v&#x3D;0的点二维泰勒展开</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324155947848.png"
                     
                ></p>
<p>将上述泰勒展开计算并简化后得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324160030776.png"
                     
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611221622674.png"
                      alt="image-20250611221622674" style="zoom:67%;" 
                >



<p>上述公式中$I_x$、$I_y$分别表示点在x方向和y方向的偏导，M是一个二阶矩矩阵加权求和；</p>
<p>从矩阵M的公式可以知道，给定一个像素区域即可将矩阵M计算出来，而矩阵M决定了u,v和E(u,v)方程之间的特性，因此将上述问题转换为分析矩阵M；</p>
<p>将上述E(u,v)与u,v的函数方程<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324160909716.png"
                     
                ></p>
<p>绘制如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250611222043663.png"
                      alt="image-20250611222043663"
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611222127050.png"
                      alt="image-20250611222127050" style="zoom: 67%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611222208525.png"
                      alt="image-20250611222208525" style="zoom:67%;" 
                >





<p>该图像沿平行uOv平面截取得到的截面是一个椭圆，这个椭圆可以看作是M矩阵的几何形状：</p>
<ul>
<li>当$I_x$和$I_y$均为0时（表现为窗口沿着任意方向移动前后差异值为0）截面为圆，表示此时窗口位于图像内部；</li>
<li>当窗口沿某一方向的导数为零时（表现为窗口沿着该方向移动前后差异值为0），截面为一个“正椭圆”，此时窗口位于边缘；</li>
<li>当窗口中含有角点（即窗口位于角上），截面椭圆的形状反映了当前窗口中角的特性；</li>
</ul>
<p>具体对某一个椭圆进行分析，根据M矩阵的形式可以知道椭圆的半长轴反应的是沿该方向导数变化的快慢，半长轴越长则导数变化越快（正椭圆的情况下）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324161411080.png"
                     
                ></p>
<p>可以看到上述椭圆并不是正椭圆，即x方向的变化与y方向的变化并不能正交（无关），表现在M矩阵中即I<del>x</del>I<del>y</del>和I<del>y</del>I<del>x</del>都不为0，则不能使用前面三种判断方式，简单的，可以使用正交矩阵R对椭圆进行旋转使其成为正椭圆</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324161638852.png"
                     
                ></p>
<blockquote>
<p>PS：此处的$λ_1$和$λ_2$分别代表了$I_x$和$I_y$；</p>
</blockquote>
<p>利用上面定义的$λ_1$和$λ_2$可以定义角点响应函数R</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324162802390.png"
                     
                ></p>
<p>即将$λ_1$和$λ_2$特征转化给R，最终检测某个窗口内是否含有角点的问题就转化为判断R的问题</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324162924445.png"
                     
                ></p>
<h3 id="4-3-Harris检测器"><a href="#4-3-Harris检测器" class="headerlink" title="4.3 Harris检测器"></a>4.3 Harris检测器</h3><blockquote>
<p>Invariance:特征提取器对进行变换过后的图像（光照、角度）提取到的特征和没有变换之前的图像提取到的特征完全相同；</p>
<p>Covariance:特征提取器对进行变换过后的图像（光照、角度）提取到的特征需要经过一定的处理变换，处理后的特征等于没有变换之前的图像提取到的特征；</p>
</blockquote>
<p>拥有了角点检测算法以后，就可以将其与其他方法结合得到特征点检测器，一个经典的特征点检测器为Harris检测器，其基本工作原理为</p>
<ol>
<li>计算每个像素处的高斯导数</li>
<li>计算每个像素周围的高斯窗口中的二阶矩矩阵M</li>
<li>计算角点响应函数R</li>
<li>设置门限R</li>
<li>寻找响应函数的局部最大值(非最大抑制)</li>
</ol>
<p>Harris检测器有如下特性：</p>
<ol>
<li>当光线强度，明暗、像素改变时，只是改变了部分角点的值，还有大部分的点可以用于检测，可以进行检测(Partially invariance)；</li>
<li>当改变位置，角度时，没有改变相对位置，可以检测(Covariance)；</li>
<li>当改变窗口大小时，大窗口下是角点，而小窗口下是线或者边缘，无法检测，此时需要使用其他方法(Not invariance or covariance)；</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230324163340955.png"
                     
                ></p>
<h2 id="5-区域检测-Blob检测"><a href="#5-区域检测-Blob检测" class="headerlink" title="5.区域检测-Blob检测"></a>5.区域检测-Blob检测</h2><p>参考文章：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107489004?spm=1001.2014.3001.5502" >(5条消息) 区域检测——Blob检测_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<hr>
<p>上面说到Harris检测器无法拟合尺度问题，Blob检测的目标是独立检测同一个图像不同缩放版本的对应区域</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331152235053.png"
                     
                ></p>
<h3 id="5-1-拉普拉斯核"><a href="#5-1-拉普拉斯核" class="headerlink" title="5.1 拉普拉斯核"></a>5.1 拉普拉斯核</h3><blockquote>
<p>拉普拉斯核具备尺度不变性和旋转不变性，它作为工具与其他算法构成比如说Harris-Laplacian或SIFT算法，这些算法是具备检测尺度不变性特征点的；至于为什么使用拉普拉斯算子而不是高斯一阶偏导核，因为只有高斯二阶偏导才能做尺度选择；</p>
</blockquote>
<p>拉普拉斯核实际就是边缘检测中提到的[高斯一阶偏导核](#2.4 高斯偏导核)的二阶偏导版本</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331152648373.png"
                     
                ></p>
<p>对不同的图像使用同一个拉普拉斯核(方差为1)进行卷积，得到如下的波纹图</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331152944652.png"
                     
                ></p>
<p>波纹图中二阶偏导为0的点表示信号边缘，即波纹表示边缘信息，当波纹重叠并出现极值（最后一幅图），表示此时的信号和拉普拉斯核对应（尺度选择特性）；</p>
<p>在原始信号发生<strong>跳变（边缘）的地方，卷积后的信号会出现一个过零点（zero-crossing）</strong>，即信号从正值变为负值（或从负值变为正值）的地方。这个过零点的位置通常被用来指示边缘。</p>
<p>利用上述特性，在实际使用过程中利用拉普拉斯模板去匹配信号，即不断改变Laplacian的参数σ，取处理后的结果达到峰值时的σ，然而简单的增大σ会导致信号卷积后特征消失（因为高斯偏导的面积公式中的σ在分母）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230331153740836.png"
                     
                ></p>
<p>为了防止这种情况需要乘以σ^2^（拉普拉斯核是二阶高斯导数），称为尺度标准化，将标准化后的拉普拉斯核用于卷积信号图像得到如下波纹图</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331154006007.png"
                     
                ></p>
<p>为了解决 LoG 算子响应强度随 σ 增大而减弱的问题，引入了<strong>尺度标准化（scale normalization）</strong>。</p>
<p>方法是：将 LoG 算子乘以 σ2。这样可以补偿因 σ 增大导致的响应衰减，使得不同尺度下的边缘响应强度具有可比性。</p>
<p>归一化后的 LoG 算子被称为<strong>尺度归一化拉普拉斯算子</strong>或<strong>尺度归一化 LoG</strong>。</p>
<h3 id="5-2-二维Blob检测"><a href="#5-2-二维Blob检测" class="headerlink" title="5.2 二维Blob检测"></a>5.2 二维Blob检测</h3><p>这里我们假设讨论的二维信号都是圆信号（之后会讨论非圆信号，由简到难），用于二维信号的拉普拉斯核的表达式及其图像如下（标准化需要乘以σ^2^）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331155840210.png"
                     
                ></p>
<p>要使用上述拉普拉斯核对二维圆信号进行检测，拉普拉斯核的方差σ与圆信号的半径有什么关系呢？换句话说，当拉普拉斯核与圆信号匹配得到最大相应的时候，拉普拉斯核与圆信号有什么联系？</p>
<p>为了得到最大相应（即拉普拉斯核与圆信号卷积后的结果最大），拉普拉斯核的零点需要与圆信号对齐 &lt;&#x3D;&gt; 卷积后响应最大，此时拉普拉斯核的零点与圆信号的边缘是对齐的；</p>
<p>当拉普拉斯核的零点精确地对准图像信号的边缘时，卷积操作会产生最大的响应</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331160316223.png"
                     
                ></p>
<p>这点比较难以理解，不妨用一维的拉普拉斯核与一维信号解释，只有形式如下的拉普拉斯核与一维信号做卷积得到的结果最大（拉普拉斯核零点与信号边缘对齐），也就是出现波纹重叠的极值，否则正向面积均会被抵消一部分；</p>
<p>这也解释了为什么原始信号与不匹配的拉普拉斯模板进行卷积后不会得到最大响应值；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331160348936.png"
                     
                ></p>
<p>二维平面中拉普拉斯核的零点方程表示为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331161110758.png"
                     
                ></p>
<p>化简后得到</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250610205652571.png"
                      alt="image-20250610205652571" style="zoom:80%;" 
                >





<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331161133932.png"
                     
                ></p>
<p>即圆半径r与拉普拉斯核的方差之间的关系为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331161203038.png"
                     
                ></p>
<blockquote>
<p>多尺度理论：拉普拉斯核的方差越大则检测的圆信号越大，即大σ检测大信号（宽阔、更模糊的边缘或较大的圆形），小σ检测小信号；</p>
</blockquote>
<p>有了上述定义后，将图像的特征尺度r定义为拉普拉斯峰值对应的尺度的根号2倍；</p>
<p>下面举个例子表示拉普拉斯核的具体应用，特征尺度选择过程中将逐步增加参数σ，每个σ 逐像素计算最大响应，每相邻取九个像素取响应值最大的像素，再与上下两层不同尺度的最大相应取最大（即在一个3x3x3共27个的响应值中取最大的响应值对应的像素点和尺度值，这是一种非最大化抑制的思想） – 同一个像素点很可能画出多个圆信号，因为方差越大圆半径越大；</p>
<p>与Canny边缘提取类似，NMS都是为了<strong>精确定位特征（边缘或斑点）消除冗余检测</strong>。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331162706577.png"
                     
                ></p>
<blockquote>
<p>不变性：拉普拉斯核也就是拉普拉斯特征点具备尺度不变性和旋转不变性，因此是invariant的(拉普拉斯特征点无论是缩放还是旋转都是特征点)；但Blob圆的大小会因为缩放和图像旋转改变，因此是covariant的；</p>
</blockquote>
<h3 id="5-3-SIFT算法"><a href="#5-3-SIFT算法" class="headerlink" title="5.3 SIFT算法"></a>5.3 SIFT算法</h3><p>这个算法的理论难度非常大，所以额外参考了其他教程：</p>
<ul>
<li>博客：[<a class="link"   target="_blank" rel="noopener" href="https://www.jianshu.com/p/95c4890c486b" >转]SIFT特征原理详解及实现 - 简书 (jianshu.com)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</li>
</ul>
<hr>
<p>在实际运用过程中，使用Laplacian核可以很好的处理尺度变换的问题，但是需要大量的计算，有两种主流的解决方法：</p>
<ul>
<li>一种是将Harris与拉普拉斯核结合，只需要在Harris角点周围是否存在尺度空间；</li>
<li>另一种是SIFT尺度不变特征；</li>
</ul>
<p>SIFT全称为Scale Invariant Feature Transform尺度不变特征变换，SIFT特征对于旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征；</p>
<p>SIFT特征检测的步骤主要分为如下四个步骤：</p>
<p><strong>尺度空间的极值检测</strong></p>
<p>搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点；</p>
<p><strong>原理：</strong> 这一步是为了在不同尺度下找到图像中的潜在兴趣点。SIFT通过构建高斯差分（DoG）金字塔来实现尺度空间。DoG图像是通过将高斯平滑图像与不同尺度的高斯核进行卷积后相减得到的。局部极值点（最大值或最小值）在DoG图像中被认为是潜在的关键点。</p>
<p><strong>步骤：</strong></p>
<p><strong>构建图像金字塔：</strong></p>
<ul>
<li>首先，将原始图像进行下采样（例如，每层尺寸减半），创建一系列不同分辨率的图像。这构成了图像金字塔的“组”（octave）。</li>
<li>在每个组中，对同一分辨率的图像进行不同程度的高斯平滑（例如，使用不同大小的高斯核）。这将创建每个组中的“层”（interval）。</li>
<li>这样做就得到了一个多尺度、多分辨率的图像集合。</li>
</ul>
<p><strong>计算高斯差分（DoG）图像：</strong></p>
<ul>
<li>对于金字塔中的每一组，将相邻两层高斯模糊图像相减，得到一系列DoG图像。例如，<code>DoG(x,y,σ) = G(x,y,kσ) - G(x,y,σ)</code>，其中G是高斯模糊函数，k是一个常数。</li>
</ul>
<p><strong>查找局部极值点：</strong></p>
<ul>
<li>在DoG图像中，检查每个像素点，并将其与它周围的26个点（同一尺度的8个邻域点，以及上下两个尺度各9个邻域点）进行比较。</li>
<li>如果某个像素点是这26个点中的最大值或最小值，那么它就被认为是一个潜在的“关键点候选”。</li>
</ul>
<p><strong>特征点定位</strong></p>
<ul>
<li><p>在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度；</p>
</li>
<li><p><strong>原理：</strong> 上一步找到的只是潜在的关键点，它们可能包含大量的边缘点或低对比度点，这些点不稳定且不具有代表性。这一步通过拟合三维二次函数来精确确定关键点的位置（x, y）和尺度（σ），并去除不稳定的关键点。</p>
<ol>
<li><strong>精确拟合：</strong></li>
</ol>
<ul>
<li>对于每一个在DoG尺度空间找到的极值点，使用泰勒展开式对DoG函数进行拟合，以获得更精确的极值位置。<ul>
<li>通过求导并令导数等于零，可以找到局部极值的精确坐标（亚像素级）和尺度。</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>去除不稳定的关键点：</strong><ul>
<li><strong>低对比度点去除：</strong> 如果拟合出的极值点的DoG响应值（也就是对比度）低于某个阈值，则认为该点不稳定，将其舍弃。例如，在一片均匀的背景中，即使算法找到了极值点，其对比度也可能非常低，不适合作为特征点。</li>
<li><strong>边缘效应去除：</strong> 边缘点在DoG函数上通常表现为沿着边缘方向的较大响应，但在垂直于边缘的方向上响应较小。SIFT使用Hessian矩阵来计算主曲率比率。如果主曲率比率超过某个阈值，则认为该点是边缘点，将其舍弃。这是因为边缘点在不同尺度下容易发生偏移，导致不稳定性。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><strong>特征方向赋值</strong></p>
<p>基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性；</p>
<ul>
<li><p><strong>原理：</strong> 这一步的目的是赋予每个关键点一个或多个主方向，从而实现旋转不变性。SIFT通过计算关键点周围区域的梯度方向直方图来确定主方向。</p>
<p><strong>案例实现：</strong></p>
<ol>
<li><strong>计算梯度幅值和方向：</strong><ul>
<li>以关键点为中心，在它所处的尺度上（也就是对应高斯模糊图像的同一尺度上）取一个局部邻域（例如，一个16x16的窗口）。</li>
<li>对该邻域内的每个像素点，计算其梯度幅值和梯度方向。梯度幅值表示像素点亮度变化的强度，梯度方向表示亮度变化的方向。</li>
</ul>
</li>
<li><strong>构建方向直方图：</strong><ul>
<li>将这个邻域内的梯度方向分为36个bin（每个bin代表10度），构建一个方向直方图。</li>
<li>每个bin的高度由落入该bin的梯度的幅值加权累加。距离关键点越近的像素点，其梯度对直方图的贡献越大（通常使用高斯加权函数）。</li>
</ul>
</li>
<li><strong>确定主方向：</strong><ul>
<li>直方图中最高的峰值代表了该关键点的主方向。</li>
<li>如果存在其他峰值，其高度超过主峰值的80%，则认为该关键点有多个主方向，为每个方向创建一个新的关键点。这使得SIFT在处理旋转目标时更加鲁棒。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><strong>特征点描述</strong></p>
<p>在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换；</p>
<p>在介绍SIFT算法之前先介绍DoG模板，一种拉普拉斯模板的替换版本；DoG的函数图像与Laplacian核很相似，具有相似的性质，但使用的是两个高斯差分来定义，大的高斯核可以使用小的高斯核来计算，大大减少了计算量</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331164648607.png"
                     
                ></p>
<p>DoG主要从以下几方面提升拉普拉斯核的效率（后面还会详细解释，这里仅给出结论）：</p>
<ol>
<li>高斯空间中的模板利用DoG算法直接在前一层的基础上计算，这样就形成一个DoG空间，得到的模板与高斯空间相差一个常数项(k-1)；</li>
<li>计算大尺度的模板时不改变参数值，而改变图像大小，例如：将图像缩小一倍时不改变模板尺度得到效果和不改变图像大小时增大模板尺度的效果相同，那么计算四倍尺度的值就可以直接将图像缩小四倍；</li>
<li>k&#x3D;2^1&#x2F;s^，其中s表示要输出的尺度数量，即可利用s来确定k；</li>
<li>模板尺度通常取2的等比数列；</li>
</ol>
<h4 id="5-3-1-尺度空间"><a href="#5-3-1-尺度空间" class="headerlink" title="5.3.1 尺度空间"></a>5.3.1 尺度空间</h4><p>在未知场景中计算机视觉并不能提供物体的尺度大小，其中一种方法是将物体不同尺度下的图像都提供给机器，让机器能够对物体在不同的尺度下有一个统一的认知，因此需要考虑图像在不同尺度下都存在的特征点；</p>
<h5 id="1-图像金字塔"><a href="#1-图像金字塔" class="headerlink" title="(1)图像金字塔"></a>(1)图像金字塔</h5><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331183732189.png"
                      alt="多分辨率图像金字塔"
                ></p>
<p>降采样：<strong>减少图像中像素的数量</strong>，从而减小图像的尺寸和数据量</p>
<p>早期图像的多尺度通常用图像金字塔表示，图像金字塔是同一图像在不同分辨率下得到的一组结果，其生成步骤主要分为两步：</p>
<ol>
<li>对原始图像进行平滑操作；</li>
<li>对处理后的图像进行降采样（一般是水平、垂直方向的1&#x2F;2像素）</li>
</ol>
<h5 id="2-高斯金字塔"><a href="#2-高斯金字塔" class="headerlink" title="(2)高斯金字塔"></a>(2)高斯金字塔</h5><p>降采样后得到一系列不断尺寸缩小的图像，因为使用的是降采样方案，故图像的局部特征难以保持，也就是无法保持特征的尺度不变性；</p>
<p>因此提出了另一种尺度空间的表现形式 – 高斯尺度空间，在分辨率不变的条件下使用不同的参数来模糊图像；</p>
<p>图像和高斯函数进行卷积可以对图像进行模糊，因此使用不同的高斯核可以得到不同模糊程度的图像，一幅图像的高斯尺度空间可以由该图像和不同的高斯卷积得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331184327817.png"
                     
                ></p>
<p>其中的G(x,y,σ)是高斯核函数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331184412063.png"
                     
                ></p>
<p>称方差σ为尺度空间因子，因为它能够反应图像被模糊的程度，σ值越大图像越模糊，对应的尺度L也就越大，由不同尺度L构成的L(x,y,σ)就是图像的高斯尺度空间；</p>
<p>构建尺度空间是为了能够检测出在不同尺度下都存在的特征点，检测特征点使用的算子就是前面介绍过的拉普拉斯核</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331184748286.png"
                     
                ></p>
<p>因为拉普拉斯核LoG的运算量过大，所以引入了DoG差分高斯来近似计算LoG</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331185025026.png"
                     
                ></p>
<p>其中k是相邻两个高斯尺度空间的比例因子，L(x,y,σ)是图像的高斯尺度空间；</p>
<p>DoG的计算公式表明将相邻的两个高斯空间的图像相减就可以得到DoG的响应图像，因此要得到DoG图像（因为得到DoG图像实际就得到了LoG），首先需要构建高斯尺度空间：高斯尺度空间可以在图像金字塔降采样的基础上加上高斯滤波得到，也就是对图像金字塔的每层图像使用不同的参数σ进行高斯模糊，使每层金字塔有多张高斯模糊过的图像；</p>
<blockquote>
<p>PS：降采样的过程中，金字塔上面一组的图像的第一张由其下面一组图像的倒数第三张降采样得到（简单理解就是因为上一组图像的最底层图像由下一组中尺度为2σ的图像进行因子为2的降采样得到）</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331191040936.png"
                      alt="高斯尺度空间"
                ></p>
<p>从上面的高斯金字塔可以看出，高斯金字塔有多组，每组有多层，一组中的多个层之间的尺度是不一样的（即使用的高斯参数σ是不同的），相邻两层之间的尺度相差一个比例因子k；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331192056885.png"
                     
                ></p>
<p>假如高斯金字塔的每组有S层，则</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331191401633.png"
                     
                ></p>
<p>高斯金字塔的组数一般为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331191802797.png"
                     
                ></p>
<p>其中o表示高斯金字塔的组数，m，n分别是图像的行和列，系数a可以是0-log<del>2</del>min(m,n)中的任意值，与具体需要的金字塔的顶层图像的大小有关，一般取值为3；</p>
<p>高斯模糊参数σ由下面的关系式得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331192652294.png"
                     
                ></p>
<p>其中o为所在的组，s为所在的层，σ<del>0</del>为初始的尺度，S为每组的层数；</p>
<p>同一组内相邻层之间的图像尺度关系为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331192847962.png"
                     
                ></p>
<p>相邻组之间的尺度关系为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331192910771.png"
                     
                ></p>
<h4 id="5-3-2-DoG空间极值检测"><a href="#5-3-2-DoG空间极值检测" class="headerlink" title="5.3.2 DoG空间极值检测"></a>5.3.2 DoG空间极值检测</h4><p>为了寻找尺度空间的极值点，每个像素点要和其图像域（同一尺度空间）和尺度域（相邻的尺度空间）的所有相邻点进行比较，当其大于（或者小于）所有相邻点时，这个点就是极值点。如图所示，中间的检测点要和其所在图像的3×3邻域8个像素点，以及其相邻的上下两层的3×3领域18个像素点，共26个像素点进行比较（非最大化抑制）；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331193221425.png"
                     
                ></p>
<p>因为每组图像的第一层和最后一层无法进行比较取得极值，为了满足尺度变换的连续性，需要在每一组图像的顶层继续使用高斯模糊生成3幅图像，因此高斯金字塔每组有S+3层图像，DoG金字塔每组有S+2层图像；</p>
<blockquote>
<p>尺度变换连续性：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230331193658622.png"
                     
                ></p>
</blockquote>
<h4 id="5-3-3-过滤特征点"><a href="#5-3-3-过滤特征点" class="headerlink" title="5.3.3 过滤特征点"></a>5.3.3 过滤特征点</h4><p>通过比较检测得到的DoG局部极值点是在离散空间搜索得到，由于离散空间是对连续空间采样得到的结果，因此离散空间上得到的极值点不一定是真正意义上的极值点，需要将不满足条件的极值点剔除；要剔除掉的不符合要求的点主要有两种：</p>
<ol>
<li>低对比度的特征点</li>
<li>不稳定的边缘响应点</li>
</ol>
<p>一般通过尺度空间DoG函数进行曲线拟合寻找极值点，这一步的本质是去掉DoG局部曲率非常不对称的点；</p>
<blockquote>
<p>剔除低对比度的特征点</p>
</blockquote>
<p>候选特征点x，其偏移量定义为Δx，其对比度为D(x)的绝对值∣D(x)∣，对D(x)应用泰勒展开式</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408223749755.png"
                     
                ></p>
<p>由于x是D(x)的极值点，所以对上式求导并令其为0，得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408223808777.png"
                     
                ></p>
<p>然后再把求得的Δx代入到D(x)的泰勒展开式中</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408223828378.png"
                     
                ></p>
<p>设对比度的阈值为T，若∣D(x^)∣≥T，则该特征点保留，否则剔除该特征点；</p>
<blockquote>
<p>删除不稳定的边缘响应点</p>
</blockquote>
<p>在边缘梯度的方向上主曲率值比较大，而沿着边缘方向则主曲率值较小。候选特征点的DoG函数D(x)的主曲率与2×2Hessian矩阵H的特征值成正比</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408224006249.png"
                     
                ></p>
<p>其中，D<del>xx</del>,D<del>xy</del>,D<del>yy</del>是候选点邻域对应位置的差分求得的。为了避免求具体的值，可以使用H特征值得比例。设α&#x3D;λmax为H的最大特征值，β&#x3D;λmin为H的最小特征值，则</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408224040630.png"
                     
                ></p>
<p>其中，Tr(H)为矩阵H的迹，Det(H)为矩阵H的行列式。</p>
<p>设γ&#x3D;α&#x2F;β表示最大特征值和最小特征值的比值，则</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408224100191.png"
                     
                ></p>
<p>上式的结果与两个特征值的比例有关，和具体的大小无关，当两个特征值想等时其值最小，并且随着γ的增大而增大。因此为了检测主曲率是否在某个阈值Tγ下，只需检测</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230408224116504.png"
                     
                ></p>
<p>如果上式成立，则剔除该特征点，否则保留特征点；</p>
<blockquote>
<p>过滤特征点之后SIFI算法还有求取特征点的主方向、生成特征点描述等步骤，但是因为难度较大暂时先放一放，感兴趣参考[<a class="link"   target="_blank" rel="noopener" href="https://www.jianshu.com/p/95c4890c486b" >转]SIFT特征原理详解及实现 - 简书 (jianshu.com)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>自学；</p>
</blockquote>
<h3 id="5-4-SIFI延展"><a href="#5-4-SIFI延展" class="headerlink" title="5.4 SIFI延展"></a>5.4 SIFI延展</h3><p>经过前面的介绍可以知道，经过SIFI算法可以利用DoG将图像中的特征点提取出来，解决了尺度的问题，接下来还存在方向、光照和视角的问题，本节介绍SIFI算法如何解决这些问题；</p>
<h4 id="5-4-1-视角变化"><a href="#5-4-1-视角变化" class="headerlink" title="5.4.1 视角变化"></a>5.4.1 视角变化</h4><p>当视角改变时，即使是同一个圆，其中的内容也有很大的差异，因此需要对其进行处理；</p>
<p>具体做法就是借助M矩阵：</p>
<ol>
<li>先确定一个圆</li>
<li>将圆内的所有像素拿出来计算M矩阵</li>
<li>比较计算出来的λ<del>1</del>、λ<del>2</del></li>
<li>将较小的λ的方向进行缩小</li>
<li>再将上一步缩小后的区域（椭圆）内的像素拿出来计算M矩阵</li>
<li>重复上述步骤，逐步迭代。直至λ<del>1</del>和λ<del>2</del> 近似相等，说明区域边缘的梯度变化近似一致</li>
<li>将椭圆转换到一样大小的圆中</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230409000344662.png"
                     
                ></p>
<h4 id="5-4-2-方向变化"><a href="#5-4-2-方向变化" class="headerlink" title="5.4.2 方向变化"></a>5.4.2 方向变化</h4><p>通过仿射自适应变换后，内容基本一致，但方向不同，对应的像素差异较大，无法识别；</p>
<p>使用梯度方向法对圆的方向进行调整：</p>
<ol>
<li>计算圆内每个像素的梯度强度和方向</li>
<li>将梯度方向量化成八份，给对应的直方图投票，票数就是梯度的大小</li>
<li>统计完之后选择票数最高的方向作为圆内像素整体的梯度方向，将方向转换到0°，将整个圆进行相同的旋转</li>
</ol>
<h4 id="5-4-3-光照变换"><a href="#5-4-3-光照变换" class="headerlink" title="5.4.3 光照变换"></a>5.4.3 光照变换</h4><p>将圆均分成16格，每个格代表一个区域，统计每个区域的方向量化梯度（两化成八个角度，长度代表梯度大小），每个区域中由一个“8位”向量表示，将16个区域的向量拉直就得到一个8*16&#x3D;128的向量来描述这个圆内的内容，最后比较每个圆的128个数来判断两个圆内容的相似程度</p>
<h2 id="6-纹理表示"><a href="#6-纹理表示" class="headerlink" title="6.纹理表示"></a>6.纹理表示</h2><p>参考链接：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107590894?spm=1001.2014.3001.5502" >纹理表示(Texture)_使用高斯偏导核,对图像_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<hr>
<p>纹理是指一些基元以某种方式组合起来的，看起来很乱但存在一定内部规律的，物体上的花纹或线条，纹理主要分为规则的纹理和不规则的纹理</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230409202615406.png"
                     
                ></p>
<h3 id="6-1-纹理描述"><a href="#6-1-纹理描述" class="headerlink" title="6.1 纹理描述"></a>6.1 纹理描述</h3><p>因为纹理是由某些重复的局部模块组成的，所以纹理描述主要分为两个部分：</p>
<ol>
<li>找到纹理的局部模块</li>
<li>描述每个局部模块的统计信息</li>
</ol>
<p>对纹理模块的提取主要使用的是前面介绍的高斯核，这里拓展一下多元高斯的概念，不同的协方差矩阵Σ对应了不同高斯核形状</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230409203000952.png"
                     
                ></p>
<p>经过不同的变化得到的filter模板如下，形成了filter bank</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230409204135135.png"
                     
                ></p>
<p>filter bank中需要添加的filter主要包括检测edges、bars、spots这几类纹理模块的功能，同时这些filter可以检测满足不同比例和方向的纹理模块的要求；</p>
<p>拥有filter bank之后，就可以按照如下步骤对图像进行纹理提取和分类：</p>
<ol>
<li>使用高斯偏导核，对图像进行卷积，x方向的偏导得到的是竖直纹理，y方向的偏导得到的是水平纹理（其他方向的偏导得到对应的纹理）；</li>
<li>统计各个方向的纹理数量，在图中表示出来，不同的区域映射的是不同的纹理特性；</li>
<li>对统计结果进行K均值聚类即可对图像的纹理类别进行区分；</li>
<li>不同窗口内的点的距离显示了窗口之间的纹理的区别；<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230409205734573.png"
                     
                ></li>
</ol>
<h3 id="6-2-纹理匹配"><a href="#6-2-纹理匹配" class="headerlink" title="6.2 纹理匹配"></a>6.2 纹理匹配</h3><p>将对应的卷积核的响应结果求均值，将所得结果组成一个7维度向量，每个向量对应一个纹理，可以将响应结果与纹理进行匹配</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230409210909630.png"
                     
                ></p>
<p>实际运用中将采取的纹理与数据库中的纹理进行对比，实现纹理检索与分类</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230409211159450.png"
                     
                ></p>
<h2 id="7-图像分割"><a href="#7-图像分割" class="headerlink" title="7.图像分割"></a>7.图像分割</h2><p>参考链接：<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/moonoa/article/details/107712114?spm=1001.2014.3001.5502" >(11条消息) 图像分割(Segmentation)——K-Means, 最小割, 归一化图割_原始 k-means 分割_Wang Yuexin的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<hr>
<p>图像分割指的是将图片相似的部分分割成相同的块，也就是将图像中相似的像素组合在一起</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230413211629393.png"
                     
                ></p>
<p>图像分割的结果可能有过分割（分割的过于细致）和欠分割（分割的不够细致），这两种情况都不是我们想要的，因此需要有一个适当的图像分割算法；</p>
<h3 id="7-1-Gestalt理论"><a href="#7-1-Gestalt理论" class="headerlink" title="7.1 Gestalt理论"></a>7.1 Gestalt理论</h3><p>分割的依据 – Gestalt理论，相似性、连通、平行…</p>
<p>Gestalt理论是解释图像分割的底层理论，即将同一个群组中的图像汇聚在一起的时候，这些图像根据相互之间的关系可以产生新的属性</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413212713503.png"
                     
                ></p>
<p>Gestalt一些常见的分组情况如下，代表了在什么样的情况下群组可以自动汇聚，什么样的情况下群组无法自动汇聚（即能够一眼辨别出的群组类型&#x2F;高效群组类型）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413212457140.png"
                     
                ></p>
<p>**都很均匀，无法分割 (Not grouped)**：元素分布均匀，没有明显的组织结构。</p>
<p>**部分靠近，可以分割 (Proximity)**：距离相近的元素倾向于被视为一个组。</p>
<p>**颜色相似性 (Similarity - Color)**：颜色相似的元素倾向于被视为一个组。</p>
<p>**形状相似性 (Similarity - Shape)**：形状相似的元素倾向于被视为一个组。</p>
<p>**朝向一致 (Common Fate)**：朝向或运动方向一致的元素倾向于被视为一个组。</p>
<p>**共同区域 (Common Region)**：处于同一封闭区域内的元素倾向于被视为一个组。</p>
<p>**平行 (Parallelism)**：相互平行的线条或元素倾向于被视为一个组。</p>
<p>**对称 (Symmetry)**：对称的元素倾向于被视为一个组。</p>
<p>**连续性 (Continuity)**：在方向上连续的线条或元素倾向于被视为一个组。</p>
<p>**封闭的几何形状 (Closure)**：不完整的图形，如果能够被知觉为完整的封闭形状，则倾向于被视为一个组。</p>
<p>一个低效的群组可能导致识别的困难，而一个高效的群组可以很轻易的识别，区分一个群组是否高效主要就是看其是否符合上述几类Gestalt分组</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413212648039.png"
                     
                ></p>
<p>Gestalt理论作为图像分割任务的指导理论，以下介绍的几种图像分割算法都是基于该理论；</p>
<h3 id="7-2-K-Means聚类"><a href="#7-2-K-Means聚类" class="headerlink" title="7.2 K-Means聚类"></a>7.2 K-Means聚类</h3><p>主要思想：将分割的任务转换为聚类的任务 – 相似的像素属于同一个事物</p>
<p>像素表达：每个像素使用一个多维向量表示，如三维向量(R,G,B)或五维向量(R,G,B,X,Y)。不同的像素表达方式会导致分割的结果不同，三维向量对应语义分割，五维向量对应实例分割</p>
<p>K-Means聚类算法：</p>
<ul>
<li><p>首先输入k的值，即希望将数据集经过聚类得到的k个分组；</p>
</li>
<li><p>从数据集中随机选择k个数据点作为初始大哥（质心，Centroid）；</p>
</li>
<li><p>对集合中每一个小弟，计算与每一个大哥的距离（一般选择欧氏距离），离哪个大哥距离近，就跟定哪个大哥；</p>
</li>
<li><p>这时每一个大哥手下都聚集了一票小弟，这时候召开代表大会，每一群选出新的大哥（其实是通过算法选出新的质心）；</p>
<ul>
<li><p>如果新大哥和老大哥之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），可以认为我们进行的聚类已经达到期望的结果，算法终止；</p>
</li>
<li><p>如果新大哥和老大哥距离变化很大，需要迭代3~5步骤；</p>
</li>
</ul>
</li>
</ul>
<p>优点：</p>
<ul>
<li>非常简单直观的方法；</li>
<li>可能收敛到局部最优值；</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>内存需求较大；</p>
</li>
<li><p>需要指定合理的超参数k；</p>
</li>
<li><p>对初始化的结果非常敏感；</p>
</li>
<li><p>对外点很敏感；</p>
<p>原因： K-Means 算法的核心是计算每个簇的质心，而这个质心通常是该簇内所有数据点的<strong>均值</strong>。均值的一个众所周知的特性是它对极端值（即外点）非常敏感。一个或几个离群点可以显著地拉动均值，使其偏离大部分数据点的真实中心。</p>
</li>
<li><p>只能检测圆&#x2F;球形群组；</p>
<p>K-Means 聚类算法对初始化（即选择初始的 K 个“大哥”&#x2F;质心）非常敏感，主要有以下几个原因：</p>
<ol>
<li><strong>局部最优解（Local Optima）的风险：</strong> K-Means 算法的目标是最小化所有数据点到其所属簇质心的距离平方和（也称为惯性或误差平方和）。这是一个优化问题。然而，K-Means 采用的是迭代逼近的方法，每次迭代都试图将质心移动到使其所在簇内的点到质心的距离之和最小的位置。 这个优化过程是一个“贪婪”的过程，它会收敛到一个局部最优解，而不是全局最优解。这意味着，如果初始质心选择得不好，算法可能会陷入一个次优的聚类结果，即使存在一个更好的聚类方式，算法也无法找到。</li>
<li><strong>聚类结果的不稳定性：</strong> 由于初始质心的随机性，每次运行 K-Means 算法，即使是相同的数据集和相同的 K 值，也可能得到不同的聚类结果。这是因为不同的初始质心可能会引导算法沿着不同的路径收敛到不同的局部最优解。这使得 K-Means 的结果不够稳定和可重复。</li>
<li><strong>空簇（Empty Clusters）问题：</strong> 在某些情况下，如果初始质心选择不当，或者数据分布非常稀疏，某个簇可能在迭代过程中没有任何数据点分配给它，从而导致一个“空簇”。这会使得后续的质心更新无法进行，因为没有点来计算该簇的新质心。</li>
<li><strong>收敛速度的影响：</strong> 一个好的初始化可以显著缩短 K-Means 算法的收敛时间。如果初始质心距离最终的理想质心较远，算法需要更多的迭代才能收敛，从而增加计算成本。</li>
</ol>
</li>
</ul>
<p>外点导致的结果</p>
<p><strong>产生“单点簇”或“小簇”：</strong> 离群点可能会形成一个非常小的簇，甚至是一个只包含它自己的簇。这会浪费一个 K 值，因为我们希望 K 个簇都能代表有意义的数据组。</p>
<p><strong>扭曲正常簇的形状和位置：</strong> 更常见的情况是，外点会拉动它们所属簇的质心，使得这个簇变得“扁长”或不规则，或者将其中心偏移到不自然的位置，从而影响了其他正常数据点的正确归属。这可能导致本应属于一个簇的点被分配到另一个簇，或者导致簇的分离不清晰。</p>
<h3 id="7-3-Mean-shift聚类"><a href="#7-3-Mean-shift聚类" class="headerlink" title="7.3 Mean-shift聚类"></a>7.3 Mean-shift聚类</h3><p>主要思想：可以认为就是寻找密度中心的问题，Mean Shift算法将像素的密度峰值作为初始值，在特征空间中寻找密度的模态或局部最大值；Mean-shift 算法的目标是寻找数据空间中数据点密度最高的区域，即“密度中心”或“模态”。</p>
<p>“类”（或“簇”）的定义：<strong>所有收敛到相同（或非常接近）的密度峰值的原始数据点，都被归为一个类。</strong></p>
<p>密度中心：随机选取像素点，向密度较大的区域漂移，迭代直到search window不再漂移</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413214223219.png"
                     
                ></p>
<p><strong>随机选取像素点（或数据点）：</strong> 算法的起点是随机选择一个数据点作为初始的“探测点”或“中心”。</p>
<p><strong>向密度较大的区域漂移（Mean Shift vector）：</strong> 从初始点开始，算法会计算一个“Mean Shift vector”（均值漂移向量）。这个向量指向当前“搜索窗口”内数据点密度增大的方向。本质上，它是当前窗口内所有数据点相对于中心点的平均偏移量。</p>
<ul>
<li><strong>Search window (搜索窗口)：</strong> 图中蓝色的圆圈代表“搜索窗口”。这是一个以当前中心点为圆心，具有固定半径（或可变半径）的区域。算法只考虑这个窗口内的数据点。</li>
<li><strong>Center of mass (质心&#x2F;平均位置)：</strong> 图中橙色的点和虚线表示当前搜索窗口内所有数据点的“质心”或平均位置。Mean Shift vector 就是从当前窗口中心指向这个质心的向量。</li>
</ul>
<p><strong>迭代直到 search window 不再漂移：</strong> 算法会沿着 Mean Shift vector 的方向移动当前的中心点，然后重复上述步骤：在新位置重新计算搜索窗口内的质心，并计算新的 Mean Shift vector。这个过程不断迭代，直到 Mean Shift vector 的长度非常小，意味着中心点已经移动到一个密度最高的区域，即“密度中心”，它不再继续显著漂移。</p>
<p><strong>search window（搜索窗口）会随着 Mean Shift vector 而不断漂移。</strong></p>
<p>这正是 Mean-shift 算法的核心机制。每一次迭代，算法都会执行以下步骤：</p>
<ol>
<li><strong>确定当前中心点：</strong> 假设当前搜索窗口的中心点是 Ct (在第 t 次迭代)。</li>
<li><strong>定义搜索窗口：</strong> 以 Ct 为中心，以预设的带宽（半径）定义一个搜索窗口。</li>
<li><strong>计算窗口内的质心：</strong> 在这个搜索窗口内，计算所有数据点的质心（平均位置），我们称之为 Cnew。</li>
<li><strong>计算 Mean Shift vector：</strong> Mean Shift vector 就是从 Ct 指向 Cnew 的向量 (Cnew−Ct)。</li>
<li><strong>更新中心点：</strong> 下一次迭代的中心点 Ct+1 就是 Cnew。这意味着搜索窗口的中心移动到了新的质心位置。</li>
<li><strong>重复：</strong> 以 Ct+1 为新的中心点，重新定义搜索窗口，并重复步骤 2-5，直到 Mean Shift vector 的长度小于一个预设的阈值，即中心点不再显著移动。</li>
</ol>
<p>为什么密度中心可以分割图像呢？因为像素点的所有轨迹都会通向具有相同模态的区域</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413214415735.png"
                     
                ></p>
<p><strong>最终如何聚类</strong></p>
<p>如何确定所有点的聚类呢？ Mean-shift 算法通常会这样做：</p>
<ol>
<li><strong>为每个数据点找到其收敛的密度峰值：</strong><ul>
<li>算法会遍历数据集中的所有（或大部分）点。</li>
<li>对每一个点，都将其作为 Mean-shift 过程的起始点，运行 Mean-shift 算法，直到其搜索窗口中心收敛。</li>
<li>每个点最终都会“漂移”到其最近的（或其梯度路径上的）一个密度峰值。</li>
</ul>
</li>
<li><strong>将收敛到相同峰值的点归为一类：</strong><ul>
<li>如果多个不同的初始点最终收敛到了<strong>同一个密度峰值</strong>（或非常接近的峰值，在某个阈值范围内被认为是同一个峰值），那么这些点就被归为<strong>同一个簇</strong>。</li>
<li>这意味着，虽然有许多不同的起始点，但它们最终都会“流向”同一个高密度区域的中心。</li>
</ul>
</li>
<li><strong>确定最终的簇和它们的质心：</strong><ul>
<li>算法会收集所有不同的收敛峰值点。这些不同的峰值点就是最终的<strong>簇的质心</strong>。</li>
<li>然后，每个数据点被分配到它所收敛到的那个峰值所代表的簇。</li>
</ul>
</li>
</ol>
<p><strong>总结一下过程：</strong></p>
<ol>
<li><strong>启动多个 Mean-shift 过程：</strong> 理论上，对数据集中<strong>每个点</strong>都启动一个 Mean-shift 过程。</li>
<li><strong>每个点找到自己的“归宿”：</strong> 每个点都会沿着密度梯度上升的方向，最终收敛到附近的一个局部密度峰值。</li>
<li><strong>合并相似的归宿：</strong> 实际上，很多不同的起始点会收敛到<strong>同一个</strong>密度峰值。这些收敛点会被识别为同一个簇的代表。</li>
<li><strong>最终聚类：</strong> 所有收敛到同一个最终峰值点的原始数据点，都被分到同一个簇中</li>
</ol>
<p>优点：</p>
<ul>
<li>不只是可以检测圆&#x2F;球形cluster；</li>
<li>只需要指定一个参数，即检测窗口的大小（检测窗口过小可能卡在局部最优值，太大了导致最终只会出现一个类）；</li>
<li>对外点有鲁棒性；</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>分割结果及其依赖窗口大小；</p>
</li>
<li><p>计算较复杂（一个一个像素的聚类），解决方法是在路径上window中的点一起进入一个类；</p>
</li>
<li><p>对于高维特征表现不是很好（漂移的方向不确定）；</p>
</li>
</ul>
<h3 id="7-4-Images-as-graphs"><a href="#7-4-Images-as-graphs" class="headerlink" title="7.4 Images as graphs"></a>7.4 Images as graphs</h3><p>主要思想：</p>
<ul>
<li>节点代表像素；</li>
<li>边代表每对像素之间的联系；</li>
<li>每条边都根据两个节点的亲和力或相似性进行加权；</li>
<li>找到一条边删除，将像素之间的联系切断 – 对切断的边的权值求和使其最小；</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413214926928.png"
                     
                ></p>
<p>边上的权值的定义如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230413215131130.png"
                     
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413215207549.png"
                     
                ></p>
<p>拥有了带权图之后，只需要选择一个合适的边切割方法即可；</p>
<p>最小割的思想是将权值最小的边去掉</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413215357415.png"
                     
                ></p>
<p>但是最小割倾向于将像素点单独切割出来，这将导致分割出很多“小块”，因此一般不选择最小割，而是选择归一化图割，归一化图割的核心思想如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413215531306.png"
                     
                ></p>
<p>如何具体的实现归一化图割呢？主要流程如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230413215715187.png"
                     
                ></p>
<p>推导</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611195446326.png"
                      alt="image-20250611195446326" style="zoom:80%;" 
                >

<p>第二小特征值（Fiedler value）的大小反映了图的<strong>连通性强度</strong>。它被称为代数连通性（Algebraic Connectivity）。Fiedler value 越小，意味着图越容易被分成两个部分。</p>
<p>优点：</p>
<ul>
<li>可以定义任何的方式来表示像素点；</li>
</ul>
<p>缺点：</p>
<ul>
<li>运算量非常大；</li>
<li>归一化图割更倾向于将图像分割成大小相等的部分，不适用于某些分割任务；</li>
</ul>
<h2 id="8-图像识别"><a href="#8-图像识别" class="headerlink" title="8.图像识别"></a>8.图像识别</h2><h3 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h3><h4 id="8-1-1-图像识别的分类"><a href="#8-1-1-图像识别的分类" class="headerlink" title="8.1.1 图像识别的分类"></a>8.1.1 图像识别的分类</h4><p>图像识别任务Recognition主要分为<code>图像分类</code>和<code>图像检测</code>：</p>
<ul>
<li>图像分类：为图像指定标签或类别的任务。例如，猫的图像可以被归类为“猫”，或者汽车的图像可以归类为“汽车”；</li>
<li>图像检测：识别和定位图像中的对象。这通常涉及在感兴趣的对象周围绘制边界框。例如，可以分析街道场景的图像，以检测和定位场景中的所有汽车；</li>
</ul>
<hr>
<blockquote>
<p>刚好我的项目中有这两个项目，感兴趣可以参考：<a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2023/04/28/%E9%A1%B9%E7%9B%AE_%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F/" >图像分类<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>，<a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2023/04/24/%E9%A1%B9%E7%9B%AE_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F/" >目标检测<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</blockquote>
<hr>
<p>对于图像检测来说，包括以下子任务：</p>
<ul>
<li><p>对象检测：对象检测包括识别图像中对象的位置和类别。它通常包括围绕对象绘制边界框，并为其指定标签或类别；</p>
</li>
<li><p>语义分割：语义分割包括根据图像中的每个像素所属对象的类别对其进行分类。换句话说，语义分割根据图像中每个像素所代表的对象为其分配标签。</p>
</li>
<li><p>实例分割：实例分割更进一步，识别和描绘图像中的单个对象，并为每个实例分配单独的标签或标识符。换句话说，实例分割不仅涉及检测图像中的对象，还涉及区分同一对象类的不同实例。实例分割是图像检测的一种更细粒度的版本，它通过识别和分离对象的单个实例来超越简单的对象检测；</p>
</li>
</ul>
<p>除了图像分类和图像检测这两个基本的任务以外，图像识别领域也有更高级的任务，比如事件识别和行为识别。</p>
<p>事件识别和行为识别不仅仅识别图像中的对象，还包括分析对象或个人之间的时间关系和交互。事件识别涉及识别视频或图像序列中发生的特定动作或事件，而行为识别侧重于识别和表征一段时间内的行为模式。</p>
<p>虽然事件识别和行为识别与图像分类和图像检测有关，但它们涉及对视觉数据进行更复杂和细致的分析。它们通常需要使用机器学习算法，这些算法可以随着时间的推移处理和分析视频或图像序列，而不仅仅是单个图像。因此，事件识别和行为识别被认为是视觉识别领域中更高级的任务。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429084529393.png"
                     
                ></p>
<p>因此，根据不同的目标和使用的方法，图像识别算法最终可分为如下四种：</p>
<ul>
<li>图像或视频的分类；</li>
<li>物体检测和定位；</li>
<li>估计语义和几何属性；</li>
<li>分析人类行为事件；</li>
</ul>
<h4 id="8-1-2-图像识别面临的问题"><a href="#8-1-2-图像识别面临的问题" class="headerlink" title="8.1.2 图像识别面临的问题"></a>8.1.2 图像识别面临的问题</h4><p>图像识别面临很多问题：需要识别的种类太多、视角变化影响、光照影响、尺寸变化影响、形变影响（如蹲着的猫和站立的猫，解决方法就是喂大量数据）、遮挡（解决方法是提取局部特征进行识别）、背景杂波、intra-class variationin 等。</p>
<h4 id="8-1-3-图像识别的步骤"><a href="#8-1-3-图像识别的步骤" class="headerlink" title="8.1.3 图像识别的步骤"></a>8.1.3 图像识别的步骤</h4><p>图像识别一般包含三个环节，其中第一步又包含图像表达和选择分类策略。第二步学习就是机器学习课程讲解的知识点，涉及学习批量、泛化、监督级别等，这里不再赘述。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230429085325301.png"
                     
                ></p>
<h5 id="1-图像表达和分类策略"><a href="#1-图像表达和分类策略" class="headerlink" title="(1)图像表达和分类策略"></a>(1)图像表达和分类策略</h5><p>（1）图像表达的方式有很多，但无论使用哪种，应当实现的基本目标为图像表达对视角、光照、尺寸、遮挡、杂波、形变有Invariance的特性。</p>
<p>（2）常用的分类策略可分为产生式和判别式。举例说明两者之间的区别，比如当前有任务“识别张三”：</p>
<ul>
<li>产生式的目标是画出张三，画的越像识别率越高（不需要李四的存在） – 了解内部规律并建模</li>
<li>判别式不需要知道张三和李四长什么样，只需要知道张三和李四之间的差异即可 – 找到内外部的差异并建模</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230429085909265.png"
                      alt="产生式模型"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429090003543.png"
                     
                ></p>
<p>用更加官方的话来说，“判别式建模的是后验概率，产生式建模的是先验概率和似然概率”，什么是先验概率，什么又是后验概率和似然概率呢？（实际上就是贝叶斯公式里的各部分）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429090207628.png"
                     
                ></p>
<p>常见的判别式模型有SVM、神经网络、Boosting等，常见的生成式模型有朴素贝叶斯、层次贝叶斯等。</p>
<h5 id="2-Recognition"><a href="#2-Recognition" class="headerlink" title="(2)Recognition"></a>(2)Recognition</h5><p>在确定了图像表达方式、分类策略以及学习方式后，最后就是Recognition，前面也说过识别主要分为分类和检测，分类非常的简单只需要对<code>整图</code>进行类别判断即可；检测稍难，不仅要从整图检测(detection)出物体，还需要识别(recognition)<code>窗口</code>中物体的类别。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525201913582.png"
                     
                ></p>
<p>检测任务具有滑动窗口问题。检测任务的本质就是判断图像中的每个窗口是否是待检测目标（检测任务实际上也是一个分类任务，只是在一个窗口内的分类）这就要求评判每个窗口的速度尽量快，进而实现实时检测。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429090735416.png"
                     
                ></p>
<p>滑动窗口的另一个问题是，可能有多个检测窗口都输出真值，需要使用非最大化抑制得到一个检测框。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429090800744.png"
                     
                ></p>
<h3 id="8-2-图像分类"><a href="#8-2-图像分类" class="headerlink" title="8.2 图像分类"></a>8.2 图像分类</h3><p>随着计算机与互联网技术以及数字图像获取技术的快速发展,海量的数字图像出现在互联网上及人们周边的生活中。依靠传统的人工方式对图像进行分类、组织和管理非常耗时耗力，所以希望能够通过计算机对图像中的目标内容进行自动地分析处理，从而将图像数据快速、规范、自动地进行组织、归类和管理。</p>
<p>早期的图像分类主要依赖于文本特征，采用人工方式为图像标注文本，使用的是基于文本的图像分类模式。由于图像标注需要人为地辨识并为其选定关键字，故其分类的效果不是非常理想，且耗时严重。随着计算机技术和数字化图像技术的发展，图像库的规模越来越大，人工标注的方式对图像进行分类已不可能，人们开始逐渐将研究的重点转移到基于图像内容分析的自动分类研究上。</p>
<p>基于内容的图像分类技术不需要进行人工标注的语义信息，而是直接对图像所包含的信息进行处理和分析，利用图像底层视觉特征来进行图像分类。图像分类技术研究是一个集中了机器学习、模式识别、计算机视觉和图像处理等多个研究领域的交叉研究方向。</p>
<p>图像分类任务的常用方法包括：</p>
<ul>
<li>传统机器学习方法：如分类器、决策树、随机森林等。<ul>
<li>逻辑回归：用于二分类问题，可以使用sigmoid等激活函数进行参数学习。</li>
<li>支持向量机：用于多分类问题，需要正则化，并可以使用核等技巧进行超参数优化。</li>
</ul>
</li>
<li>深度学习模型：其中最常用的是卷积神经网络，如卷积神经网络（CNN）、循环神经网络（RNN）等。</li>
</ul>
<p>本次实验要求使用支持向量机对图像进行分类，下面是使用支持向量机实现图像分类的一般步骤：</p>
<ol>
<li>数据预处理：将图像转换为二维数组，并对每个像素进行标准化处理，以消除像素之间的尺度差异。</li>
<li>构建SVM分类器：使用支持向量机算法构建SVM分类器，通常使用径向基函数（RBF）或多项式核（Polynomial Kernel）等核函数。</li>
<li>训练SVM分类器：使用训练数据训练SVM分类器，通常使用交叉验证等方法进行模型选择和参数优化。</li>
<li>使用SVM分类器进行预测：使用训练好的SVM分类器对新的图像进行分类预测。</li>
</ol>
<p>图像分类常用的是深度学习的模型（毕竟诸如CNN这种能够自动提取图像的局部特征）。但是，这并不妨碍SVM是一个优秀的算法，大量的实践都证明它对于小批量数据集的分类、预测都能够得到较好的效果。</p>
<h4 id="8-2-1-词袋模型概述"><a href="#8-2-1-词袋模型概述" class="headerlink" title="8.2.1 词袋模型概述"></a>8.2.1 词袋模型概述</h4><p>参考链接（这一节看文本很难形成直观上的理解，推荐搭配视频）：</p>
<ul>
<li><p>视频：<a class="link"   target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nz4y197Qv?p=11&vd_source=276d55048634a5b508b1b53a1ecd56b3" >09.识别&amp;词袋模型.1080P_哔哩哔哩_bilibili<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>（这个视频只能作为最基本的入门来看，很多描述不准确）；</p>
</li>
<li><p><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/wxl845235800/p/10564121.html" >视觉单词模型、词袋模型BoW - ostartech - 博客园 (cnblogs.com)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</li>
<li><p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/baidu_28563321/article/details/46348439" >(6条消息) 计算机视觉课程作业 基于词袋模型的图像分类算法_蒋_X_X的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</li>
</ul>
<hr>
<p>词袋模型BoW在NL和CV领域都有提到，这里我们主要介绍词袋模型在CV领域的使用即BoVW。</p>
<p>词袋模型最初用于文本分类中，然后逐步引入到了图像分类任务中。在文本分类中，文本被视为一些不考虑先后顺序的单词集合。而在图像分类中，图像被视为是一些与位置无关的<code>局部区域</code>的集合（一袋拼图），因此这些图像中的局部区域就等同于文本中的单词。在不同的图像中，局部区域的分布是不同的（一袋“马”的拼图肯定和一袋“牛”的拼图不同）。因此，可以利用提取的局部区域的分布对图像进行识别。</p>
<blockquote>
<p>注意这里的“局部区域”，一般指的是具有代表性的图像区域，也就是图像特征，一般使用SIFT提取器提取（当然直接将图像均分为局部区域也可以，但这种做法得到的拼图过多影响后续处理）</p>
</blockquote>
<p>图像分类和文本分类的不同点在于，在文本分类的词袋模型算法中，字典是已存在的，不需要通过学习获得；而在图像分类中，词袋模型算法需要通过监督或非监督的学习来获得视觉词典。造成这种差异的原因是，图像中的视觉特征不像自然语言中的单词那样定义明确和被理解。因此，有必要学习一种能够有效地表示图像中特征的视觉词典。</p>
<p><code>视觉词袋（BoVW，Bag of Visual Words）模型</code>，是“词袋”（BoW，Bag of Words）模型从自然语言处理与分析领域向图像处理与分析领域的一次自然推广。对于任意一幅图像，BoVW模型提取该图像中的基本元素，并统计该图像中这些基本元素出现的频率，用直方图的形式来表示。通常使用“图像局部特征”来类比BoW模型中的单词，如SIFT、SURF、HOG等特征，所以也被称之为<code>视觉单词模型</code>。</p>
<p>图像BoVW模型表示的直观示意图如图所示</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429100507214.png"
                     
                ></p>
<p>利用BoVW模型表示图像，获得图像的全局直方图表示，主要有四个关键步骤：</p>
<p>Step 1：图像局部特征提取（Image Local Features Extrication）。根据具体应用考虑，综合考虑特征的独特性、提取算法复杂性、效果好坏等选择特征。利用局部特征提取算法，从图像中提取局部特征。 – SIFT特征提取器</p>
<p>Step 2：视觉词典构造（Visual Dictionary Construction）。利用上一步得到的特征向量集，抽取其中有代表性的向量，作为单词，形成视觉词典。一般是从图像库中选取一部分来自不同场景或类别的图像来组成训练图像集，并提取其局部特征，然后对训练图像的所有局部特征向量通过适当的去冗余处理得到一些有代表性的特征向量，将其定义为视觉单词。通常所采用的处理方法是对训练图像的所有局部特征向量进行聚类分析，将聚类中心定义为视觉单词。所有视觉单词组成视觉词典，用于图像的直方图表示。 – K-means聚类</p>
<p>Step 3：特征向量量化(Feature Vector Quantization)。BoVW模型采用向量量化技术实现，向量量化结果是将图像的局部特征向量量化为视觉单词中与其距离最相似的视觉单词。向量量化过程实际上是一个搜索过程，通常采用最近邻搜索算法，搜索出与图像局部特征向量最为匹配的视觉单词。 – KNN最近邻聚类</p>
<p>Step 4：用视觉单词直方图表示图像，也称为量化编码集成(Pooling)。一幅图像的所有局部特征向量被量化后，可统计出视觉词典中每个视觉单词在该图像中出现的频数，得到一个关于视觉单词的直方图，其本质是上一步所得量化编码的全局统计结果，是按视觉单词索引顺序组成的一个数值向量（各个元素的值还可以根据一定的规则进行加权）。该向量即为图像的最终表示形式。</p>
<p>上述步骤很抽象，下面举一个例子来帮助理解：</p>
<p>现在我们有如下三张原始图像</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429100908612.png"
                      alt="原始图像"
                ></p>
<p>​							</p>
<p>第一步是做局部特征提取，假设使用SIFT特征提取器提取到了如下这些特征（可以认为是拼图，我们将三张原始图像的拼图都放在一个袋子中，这就是我们的“词袋”）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429101118048.png"
                      alt="词袋"
                ></p>
<p>第二步是视觉词典构造，也就是从上述那么多特征中选取k个单词来构成视觉词典，这里的k由人为规定（一般需要先验知识）。使用K-means聚类算法将词袋中的拼图聚为k个类，k个类别中心的拼图就是k个单词（假设我们令k&#x3D;4）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429101431211.png"
                      alt="视觉词典"
                ></p>
<p>​				</p>
<p>第三步和第四步通常合并进行，即对词袋中的每个拼图都使用最近邻算法，寻找与其最近的在视觉词典中的单词（我们认为这两张图像属于同一个单词），找到之后对直方图中该单词的频数+1。因为我们有三张原始图像，所以对应有三个直方图，每个直方图的横轴为视觉词典中的单词，纵轴为该单词出现的频数，因此对原始三张图像的直方图表示如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230429101848165.png"
                      alt="直方图表示"
                ></p>
<hr>
<blockquote>
<p>Q：K-means聚类和KNN最近邻有什么区别？</p>
</blockquote>
<p>A：回答参考<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31580379" >KNN和K-mean有什么不同？ - 知乎 (zhihu.com)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
<ul>
<li>KNN<ul>
<li><p>分类算法</p>
</li>
<li><p>监督学习</p>
</li>
<li><p>数据集是带Label的数据</p>
</li>
<li><p>没有明显的训练过程，基于Memory-based learning</p>
</li>
<li><p>K值含义 - 对于一个样本X，要给它分类，首先从数据集中，在X附近找离它最近的K个数据点，将它划分为归属于类别最多的一类</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611204317482.png"
                      alt="image-20250611204317482" style="zoom:67%;" 
                ></li>
</ul>
</li>
<li>K-means<ul>
<li>聚类算法</li>
<li>非监督学习</li>
<li>数据集是无Label，杂乱无章的数据</li>
<li>有明显的训练过程</li>
<li>K值含义- K是事先设定的数字，将数据集分为K个簇，需要依靠人的先验知识</li>
</ul>
</li>
</ul>
<hr>
<h4 id="8-2-2-词袋模型-SVM图像分类"><a href="#8-2-2-词袋模型-SVM图像分类" class="headerlink" title="8.2.2 词袋模型+SVM图像分类"></a>8.2.2 词袋模型+SVM图像分类</h4><p>词袋模型的概念介绍完毕，下面通过“词袋模型+SVM图像分类”任务来介绍其中涉及的知识点（这是一个最基本的词袋模型的运用），即SIFT特征提取器、K-means算法、KNN算法以及最终进行图像分类需要使用的SVM模型（以下知识点都是之前学习过程中自己总结的，如果有看不懂的也可以自行Google）</p>
<p>完整的项目链接在<a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2023/04/28/%E9%A1%B9%E7%9B%AE_%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F/" >图像分类<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>，涉及的知识点链接都在下面：</p>
<ul>
<li><p>[SIFT特征提取器](# 5.3 SIFT算法)；</p>
</li>
<li><p><a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2022/08/08/%E4%B8%93%E4%B8%9A_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/#2-8-K-means%E8%81%9A%E7%B1%BB" >K-means聚类<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</li>
<li><p><a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2022/08/08/%E4%B8%93%E4%B8%9A_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/#2-3-KNN%E7%AE%97%E6%B3%95" >KNN算法<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</li>
<li><p><a class="link"   target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/2022/08/08/%E4%B8%93%E4%B8%9A_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/#2-7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA" >SVM支持向量机<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</p>
</li>
</ul>
<p>要实现基于词袋模型的图像分类，大致分为如下四步：</p>
<ol>
<li>特征提取与描述子生成：一般选择SIFT特征提取器，SIFT特征具有放缩、旋转、光照不变性，同时兼有对几何畸变，图像几何变形的一定程度的鲁棒性；</li>
<li>词袋生成：词袋生成基于描述子数据的基础上，生成一系列的向量数据，最常见就是首先通过K-Means实现对描述子数据的聚类分析，一般会分成K个聚类、得到每个聚类的中心数据，就生成了K单词，根据每个描述子到这些聚类中心的距离，决定了它属于哪个聚类，这样就生成了图像的直方图表示数据。</li>
<li>SVM分类训练与模型生成：使用SVM进行数据的分类训练，得到输出模型；</li>
<li>模型使用预测：加载预训练好的模型，使用模型在测试集上进行数据分类预测；</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://gintoki-jpg.github.io/images/image-20230525164957239.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230525164957239.png"
                     
                ></a></p>
<h3 id="8-3-图像检测"><a href="#8-3-图像检测" class="headerlink" title="8.3 图像检测"></a>8.3 图像检测</h3><p>参考资料：</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/matrix_space/article/details/53840740" >(7条消息) 机器学习: Viola-Jones 人脸检测算法解析(一)_vio-jones算法_Matrix_11的博客-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</li>
<li>[(7条消息) 机器学习: Viola-Jones 人脸检测算法解析(二)_violajones算法_Matrix_11的博客-CSDN博客](<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/matrix_space/article/details/53892765?ops_request_misc=&request_id=&biz_id=102&utm_term=Viola-Jones" >https://blog.csdn.net/matrix_space&#x2F;article&#x2F;details&#x2F;53892765?ops_request_misc&#x3D;&amp;request_id&#x3D;&amp;biz_id&#x3D;102&amp;utm_term&#x3D;Viola-Jones<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 人脸检测算法解析&amp;utm_medium&#x3D;distribute.pc_search_result.none-task-blog-2<del>all</del>sobaiduweb~default-1-53892765.142^v88^insert_down38v5,239^v2^insert_chatgpt&amp;spm&#x3D;1018.2226.3001.4187)；</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1419615" >HOG特征详解与行人检测-腾讯云开发者社区-腾讯云 (tencent.com)<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>；</li>
</ul>
<h4 id="8-3-1-人脸检测"><a href="#8-3-1-人脸检测" class="headerlink" title="8.3.1 人脸检测"></a>8.3.1 人脸检测</h4><p>在人脸检测邻域，Viola-Jones算法非常经典。Viola-Jones算法在2001年的CVPR上提出，因为其高效而快速的检测即使到现在也依然被广泛使用。VJ算法包含以下几个重要的部分：</p>
<ol>
<li>利用 Haar 特征描述人脸的共有属性；</li>
<li>建立了一种称为积分图像的特征，并且基于积分图像，可以快速获取几种不同的矩形特征；</li>
<li>利用 Adaboost 算法进行训练；</li>
<li>建立层级分类器；</li>
</ol>
<h5 id="1-Harr特征"><a href="#1-Harr特征" class="headerlink" title="(1)Harr特征"></a>(1)Harr特征</h5><p>一般来说，人脸会有一些基本的共性，比如眼睛区域会比脸颊区域要暗很多、鼻子一般属于脸部的高光区域因而鼻子会比周围的脸颊要亮很多、一张正脸图像，眼睛，眉毛，鼻子，嘴巴等的相对位置是有规律可循的。__Haar特征考虑的是某一特定像素点相邻的矩形区域的特征__，这需要将整个矩形区域内的像素相加然后再做减法运算。一般的，有以下几种Harr特征算子</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525202931622.png"
                     
                ></p>
<p>普通的特征都是针对单一像素，Harr特征是针对矩形区域，因此计算时需要先将每个矩形区域的像素进行求和，然后再使用上面的算子进行计算</p>
<h5 id="2-积分图像"><a href="#2-积分图像" class="headerlink" title="(2)积分图像"></a>(2)积分图像</h5><p>为了计算Harr特征，需要先对矩形区域内的所有像素求和。而一张图像中的像素形成的矩形区域形状、大小不定，因此如果使用常规的对每个矩形区域内的像素进行遍历求和的方法会导致很大的运算量。下面引入这样一种数据结构，称为积分图像。积分图像的原理非常简单，即对积分图像中的任何一点，该点的积分图像值等于该点在原图中的点左上角的所有像素值之和，表达式如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525204712833.png"
                     
                ></p>
<p>积分图像与原图之间满足如下关系</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525204801412.png"
                     
                ></p>
<p>其中I表示积分图像，f表示原图，x,y表示像素点的坐标。因此，一张图像的积分图像记录了这张图像上每一个像素点其左上角所有像素的和。如果把一张图像的左上角看做坐标原点，那么上面的表达式就是原点到该像素点之间的所有像素点的离散求和，这可以看成是一种积分，所以这也是积分图像名称的由来。利用积分图像，可以非常快速的计算一张图像上任意一个矩形区域内的像素和</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525205220890.png"
                     
                ></p>
<p>这意味着，原图中任意一个矩形区域的像素和，都可以由上述运算法则求解。</p>
<p>在VJ人脸检测中，主要使用了三种不同的矩形特征（Harr特征中的其中三个），分别是二邻接，三邻接，四邻接矩形，矩形的Value计算公式为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230525211433397.png"
                     
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525205709327.png"
                     
                ></p>
<p>因为一个矩阵需要使用四个点来计算&#x2F;表示，进而二邻接矩形需要六个点表示，三邻接矩形需要八个点，而四邻接矩形需要九个点表示。</p>
<p>下面来计算给定一张图像，能够得到多少个矩形特征。</p>
<p>VJ算法中使用的图像大小都是<code>24*24</code>的图像大小。考虑水平方向和垂直方向二邻接矩形有两种情况1×2和2×1，三邻接矩形也有两种情况1×3和3×1，而四邻接矩形只有一种情况2×2。</p>
<p>下面列出每种矩阵可能的size：</p>
<p>根据卷积定理，一个大小为<code>W*H</code>的图像与大小为<code>m*n</code>的filter做卷积，新生成的图像大小为<code>(W-m+1)*(H-n+1)</code>，这意味着新图像有多少个像素，则对应原图有多少个<code>m*n</code>大小的矩形。</p>
<p>计算过程省略，最终一个<code>24*24</code>大小的图像会产生162336个矩形特征，这个维度远远高于图像本身的维度，因此不可能将这些特征全部使用，需要使用Boost来做特征选择与训练。</p>
<h5 id="3-Boost训练"><a href="#3-Boost训练" class="headerlink" title="(3)Boost训练"></a>(3)Boost训练</h5><p>前面已经说过，VJ人脸检测算法使用的特征就是基于积分图像的矩形特征，也称为Harr特征。一张图像生成的Harr特征集远高于图像维度，下面介绍使用Boost做特征选择和分类器训练。</p>
<p>Boost可以同时进行特征选择与分类器训练，简单来说，Boost就是将一系列的”弱”分类器通过线性组合，构成一个”强”分类器</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525220003853.png"
                     
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\大四下\计算机视觉\assets\image-20250611211139898.png"
                      alt="image-20250611211139898" style="zoom:67%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611211331718.png"
                      alt="image-20250611211331718" style="zoom:67%;" 
                >

<p>其中h(x)就是一个强分类器</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525220054527.png"
                     
                ></p>
<p>而h<del>j</del>(x)就是“弱”分类器，h<del>j</del>(x)本质上是一个简单的阈值函数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525220211292.png"
                     
                ></p>
<blockquote>
<p>规定：给定N个训练样本(x^i^,y^i^)，其中含有m个正样本，l个负样本。如果x^i^是人脸图像则y^i^&#x3D;1，否则y^i^&#x3D;0。</p>
</blockquote>
<p>在 AdaBoost 的整个过程中，<strong>门限（阈值 Tj）不是固定的</strong>，并且<strong>不同哈尔特征的门限通常是不同的</strong></p>
<p>基于Boost的训练算法具体流程如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525220830778.png"
                     
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611215720886.png"
                      alt="image-20250611215720886" style="zoom: 50%;" 
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611215808555.png"
                      alt="image-20250611215808555" style="zoom: 50%;" 
                >





<ul>
<li>θ<del>j</del>门限是在第1步确定的，简单理解就是选择什么样的θ能够使得该弱分类器效果最好（正确率最高）；</li>
<li>β是一个小于1的数，权重更新会导致权重越来越小，这意味着搞不定的样本的权值不变，搞定的权值的样本减小（变相增大搞不定的样本的权值）；</li>
<li>α权重决定了最终该弱分类器在整个Boosting组合中的重要程度，与β成反比，与分类错误率ε成正比；</li>
<li>上述步骤2，3，4会一直循环，直到生成的强分类器的性能达到要求，此时选择的弱分类器就是前面162336个矩形特征中被选中的特征（即此处的特征和弱分类器是等价的）；</li>
</ul>
<h5 id="4-层级分类器"><a href="#4-层级分类器" class="headerlink" title="(4)层级分类器"></a>(4)层级分类器</h5><p>在一张正常的图像中，包含人脸的区域只占整张图像中很小的一部分，如果所有的局部区域都要遍历所有特征的话，这个运算量非常巨大，也非常耗时，所以为了节省运算时间，应该把更多的检测放在潜在的正样本区域上。所以有了层级分类器的概念，层级分类器就是为了将任务简化，一开始用少量的特征将大部分的negative 区域剔除，后面再利用复杂的特征将 false positive 区域剔除。</p>
<p>在层级分类器架构中，每一层级含有一个“强”分类器（也就是上一步中Boosting学习得到的强分类器），同时所有的矩形特征被分成几组，每一组都包含部分矩形特征，这些矩形特征用在层级分类器的不同阶段。层级分类器的每一阶段都会判别输入的区域是不是人脸，如果肯定不是，那么这个区域会被立即舍弃掉，只有那些被判别为可能是人脸的区域才会被传入下一阶段用更为复杂的分类器进一步的判别。其流程图如下所示:</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525222059747.png"
                     
                ></p>
<ul>
<li>处于cascade层级分类器前端的分类器分类速度较快，可以拒绝大部分负样本并且检测出几乎所有的正样本（正样本一定要通过，负样本在可接受范围内可通过一定数量）；</li>
<li>处于cascade层级分类器后端的分类器分类速度较慢，但是大多数区域样本几乎无法到达；</li>
</ul>
<p>每个层级的矩形特征都比较简单。下面的两种特征可以达到100%的检测率，但是也会产生很多的 false positive，一般来说是 50%的FP rate。但是这两种特征对 negative 区域的识别非常高效，所以层级分类器的第一层基本都是用这两种特征加上一个”强”分类器先将大量的negative 区域剔除。对于 false positive 的处理，有赖于后面阶段更多的特征及分类器。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525222852635.png"
                     
                ></p>
<p>层级分类器总的识别率D或false positive F是每一层的分类器的识别率d和false positive f的乘积</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525222957256.png"
                     
                ></p>
<p>一个层级分类器，应该综合考虑以下几个因素：</p>
<ul>
<li>层级分类器的层次，即需要多少个分类器；</li>
<li>每一层分类器需要测试的特征数n<del>i</del>；</li>
<li>每一层分类器的阈值；</li>
</ul>
<h4 id="8-3-2-行人检测"><a href="#8-3-2-行人检测" class="headerlink" title="8.3.2 行人检测"></a>8.3.2 行人检测</h4><h5 id="1-HOG概述"><a href="#1-HOG概述" class="headerlink" title="(1)HOG概述"></a>(1)HOG概述</h5><p>HOG特征全称方向梯度直方图特征，在对象检测与模式匹配中是一种常见的特征提取算法，是一种基于本地像素块进行特征直方图提取的一种算法，对象局部的变形与光照影响有很好的稳定性，最初是用HOG特征来来识别人像，通过HOG特征提取+SVM训练，可以得到很好的效果。HOG特征提取的大致流程如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225326527.png"
                     
                ></p>
<p>1.灰度化：对HOG特征提取来说第一步是对输入的彩色图像转换为灰度图像，图像灰度化的方法有很多，不同灰度化方法之间有一些微小的差异，从彩色到灰度的图像转换可以表示如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225421591.png"
                     
                ></p>
<p>2.计算图像梯度：计算图像的X方向梯度dx与Y方向梯度dy，根据梯度计算mag与角度，计算梯度时候可以先高斯模糊一下(可选)，然后使用sobel或者其它一阶导数算子计算梯度值dx、dy、mag、angle</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225516084.png"
                     
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611212612008.png"
                      alt="image-20250611212612008" style="zoom:67%;" 
                >

<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20250611212625969.png"
                      alt="image-20250611212625969"
                ></p>
<p>3.Cell分割与Block对于图像来说，分成8x8像素块，每个块称为一个Cell，每个2x2大小的Cell称为一个Block，每个Cell根据角度与权重建立直方图，每20度为一个BIN，每个Cell得到9个值、每个Block得到36个值(4x9)，图像如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://gintoki-jpg.github.io/images/image-20230525225600363.png"
                     
                ></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611213634812.png"
                      alt="image-20250611213634812" style="zoom:67%;" 
                >

<p>每个Block为单位进行L2数据归一化，作用是抵消光照&#x2F;迁移影响，L2的归一化的公式如下</p>
<p>V 是一个 <strong>36 维的 Block 特征向量</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225620678.png"
                     
                ></p>
<p>4.生成描述子：对于窗口64x128范围大小的像素块，可以得到8x16个Cell， 使用Block在窗口移动，得到输出的向量总数为7x15x36&#x3D;3780特征向量，每次Block移动步长是八个像素单位，一个Cell大小</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="\计算机视觉笺疏\image-20250611214206797.png"
                      alt="image-20250611214206797" style="zoom:67%;" 
                >

<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225647957.png"
                     
                ></p>
<h5 id="2-HOG-SVM行人检测"><a href="#2-HOG-SVM行人检测" class="headerlink" title="(2)HOG+SVM行人检测"></a>(2)HOG+SVM行人检测</h5><p>基于HOG特征和SVM分类器的行人检测的基本流程如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%5C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AC%BA%E7%96%8F%5Cimage-20230525225858138.png"
                     
                ></p>
<p>训练过程中正样本为图片中包含有目标区域(行人)的boundingbox，尺寸统一为检测窗口的大小即64×128，负样本不需要统一尺寸，只需比检测窗口大，且图片中不包含检测目标，可任意截取图片中64×128大小的区域提取HOG特征作为负样本的特征向量，并与正样本图片中boundingbox区域提取出的HOG特征向量一起训练，得到SVM的分类模型。</p>
<p>检测过程中采用滑动窗口法，检测窗口尺寸固定不变，对待检测图片进行尺度缩放，在每一层的图像上，用固定大小的滑动窗口提取HOG特征，并根据训练好的分类模型判断检测窗口是否为目标(行人)。因此HOG + SVM进行行人检测的过程实际上就是对图像的检测窗口提取HOG特征进行分类判决的过程。</p>
<p>该流程分为四个核心步骤：</p>
<p><strong>1. Extract fixed-sized (64x128 pixel) window at each position and scale</strong></p>
<ul>
<li><strong>固定大小的滑动窗口：</strong> 算法首先会在图像上使用一个固定大小的检测窗口（例如 64×128 像素，这是行人检测常用的尺寸）进行滑动。这意味着它会逐个地检查图像中的所有可能的区域。</li>
<li><strong>多尺度检测：</strong> 为了检测不同大小的行人，图像本身会被缩放到多个尺度（即构建图像金字塔）。然后在每个尺度的图像上，重复使用同样大小的滑动窗口进行检测。这确保了无论行人大小如何，总有一个窗口能够以合适的尺寸“套住”它。</li>
</ul>
<p><strong>2. Compute HOG (histogram of gradient) features within each window</strong></p>
<ul>
<li><strong>HOG 特征提取：</strong> 对于在步骤 1 中提取出的每一个 64×128 像素的图像子窗口，都会计算它的 HOG 特征描述符。</li>
<li><strong>HOG 特征的计算过程：</strong> 这包括将窗口划分为 Cells（例如 8×8 像素），在每个 Cell 内计算梯度方向直方图（例如 9 个 bin），然后将这些 Cells 组合成 Block（例如 2×2 个 Cell，共 16×16 像素），并对每个 Block 进行归一化（如 L2 归一化）以增强对光照变化的鲁棒性。最终，所有归一化后的 Block 特征串联起来，形成一个高维的 HOG 特征向量（例如 7×15×36&#x3D;3780 维）。</li>
</ul>
<p><strong>3. Score the window with a linear SVM classifier</strong></p>
<ul>
<li><p><strong>SVM 分类器：</strong> 提取出 HOG 特征向量后，这个向量会被输入到一个预先训练好的线性支持向量机 (SVM) 分类器中。</p>
</li>
<li><p><strong>打分：</strong> SVM 分类器会给这个窗口打一个分数。这个分数表示该窗口包含目标物体（行人）的可能性。分数越高，表示该窗口是行人的置信度越高。</p>
</li>
<li><p>训练过程：</p>
<p> SVM 分类器是在离线阶段训练好的。训练数据包括：</p>
<ul>
<li><strong>正样本：</strong> 包含行人 (bounding box) 的图像区域，这些区域被裁剪并统一缩放到 64×128 大小，然后提取 HOG 特征作为正样本的特征向量。</li>
<li><strong>负样本：</strong> 不包含行人的图像区域。这些区域不需要统一尺寸，只要比检测窗口大且不包含目标即可。从这些区域中随机裁剪 64×128 大小的子区域，提取 HOG 特征作为负样本的特征向量。SVM 就是通过这些正负样本的 HOG 特征进行训练，学习一个将行人与非行人区分开来的决策边界。</li>
</ul>
</li>
</ul>
<p><strong>4. Perform non-maxima suppression to remove overlapping detections with lower scores</strong></p>
<ul>
<li><p><strong>非极大值抑制 (NMS)：</strong> 在步骤 1 到 3 的扫描过程中，一个行人很可能会被多个重叠的检测窗口识别出来，并得到多个较高的分数。</p>
</li>
<li><p>去除冗余：</p>
<p> NMS 算法用于解决这个问题。它会：</p>
<ol>
<li>选择分数最高的检测框。</li>
<li>抑制（移除）所有与该检测框有较大重叠（通常通过 IoU, Intersection over Union 阈值判断）且分数较低的其他检测框。</li>
<li>重复这个过程，直到所有检测框都被处理完毕。</li>
</ol>
</li>
<li><p><strong>最终输出：</strong> 经过 NMS 后，图像上只会留下每个行人最自信（分数最高）且不重叠的唯一检测框，从而得到最终的行人检测结果。</p>
</li>
</ul>
<p><strong>总结：</strong></p>
<p>这张图清晰地概述了 HOG+SVM 进行行人检测的完整流程。它是一个经典的<strong>滑动窗口 + 特征提取 + 分类器 + 后处理</strong>的模式。通过在图像金字塔上滑动固定大小的窗口，提取 HOG 特征来描述每个窗口的局部纹理和形状信息，然后用训练好的 SVM 分类器判断该窗口是否包含行人，最后通过非极大值抑制来精炼和优化检测结果，去除冗余，从而实现高效准确的行人检测。</p>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> 计算机视觉笺疏</li>
        <li><strong>Author:</strong> 2314</li>
        <li><strong>Created at
                :</strong> 2025-03-06 00:00:00</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-07-01 21:57:55
            </li>
        
        <li>
            <strong>Link:</strong> https://redefine.ohevan.com/2025/03/06/计算机视觉笺疏/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">#人工智能</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/2025/03/06/%E8%AE%A1%E7%84%B6%E4%B8%8D%E6%97%A2%E7%84%B6/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">计然笺疏（连载中）</span>
						<span class="post-nav-item">Prev posts</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2024/12/23/All%20about%20artificial%20intelligence/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">All about artificial intelligence</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="waline"></div>
    <script type="module" data-swup-reload-script>
      import { init } from '/js/libs/waline.mjs';

      function loadWaline() {
        init({
          el: '#waline',
          serverURL: 'https://example.example.com',
          lang: 'zh-CN',
          dark: 'body[class~="dark-mode"]',
          reaction: false,
          requiredMeta: ['nick', 'mail'],
          emoji: [],
          
          
        });
      }

      if (typeof swup !== 'undefined') {
        loadWaline();
      } else {
        window.addEventListener('DOMContentLoaded', loadWaline);
      }
    </script>



        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">计算机视觉笺疏</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9C%8B%E6%95%B0%E5%AD%A6"><span class="nav-text">在计算机视觉看数学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="nav-text">1.1 卷积核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%89%B9%E6%80%A7"><span class="nav-text">1.2 卷积的特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-text">1.3 卷积的功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86"><span class="nav-text">1.4 噪声处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%BE%B9%E7%BC%98"><span class="nav-text">2.边缘</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E8%BE%B9%E7%BC%98%E6%8F%90%E5%8F%96"><span class="nav-text">2.1 边缘提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E6%9C%89%E9%99%90%E5%B7%AE%E5%88%86%E6%BB%A4%E6%B3%A2%E5%99%A8"><span class="nav-text">2.2 有限差分滤波器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%9B%BE%E5%83%8F%E6%A2%AF%E5%BA%A6"><span class="nav-text">2.3 图像梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E9%AB%98%E6%96%AF%E5%81%8F%E5%AF%BC%E6%A0%B8"><span class="nav-text">2.4 高斯偏导核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Canny%E8%BE%B9%E7%BC%98%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95"><span class="nav-text">2.5 Canny边缘提取算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%8B%9F%E5%90%88"><span class="nav-text">3.拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="nav-text">3.1 最小二乘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%9D%83%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="nav-text">3.2 权最小二乘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E9%B2%81%E6%A3%92%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98"><span class="nav-text">3.3 鲁棒性最小二乘</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-RANSAC"><span class="nav-text">3.4 RANSAC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Hough-transform"><span class="nav-text">3.5 Hough transform</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B-%E8%A7%92%E7%82%B9"><span class="nav-text">4.区域检测-角点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E7%89%B9%E5%BE%81%E7%82%B9"><span class="nav-text">4.1 特征点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="nav-text">4.2 特征提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Harris%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="nav-text">4.3 Harris检测器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B-Blob%E6%A3%80%E6%B5%8B"><span class="nav-text">5.区域检测-Blob检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E6%A0%B8"><span class="nav-text">5.1 拉普拉斯核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E4%BA%8C%E7%BB%B4Blob%E6%A3%80%E6%B5%8B"><span class="nav-text">5.2 二维Blob检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-SIFT%E7%AE%97%E6%B3%95"><span class="nav-text">5.3 SIFT算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-SIFI%E5%BB%B6%E5%B1%95"><span class="nav-text">5.4 SIFI延展</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E7%BA%B9%E7%90%86%E8%A1%A8%E7%A4%BA"><span class="nav-text">6.纹理表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E7%BA%B9%E7%90%86%E6%8F%8F%E8%BF%B0"><span class="nav-text">6.1 纹理描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E7%BA%B9%E7%90%86%E5%8C%B9%E9%85%8D"><span class="nav-text">6.2 纹理匹配</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="nav-text">7.图像分割</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Gestalt%E7%90%86%E8%AE%BA"><span class="nav-text">7.1 Gestalt理论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-K-Means%E8%81%9A%E7%B1%BB"><span class="nav-text">7.2 K-Means聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-Mean-shift%E8%81%9A%E7%B1%BB"><span class="nav-text">7.3 Mean-shift聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-Images-as-graphs"><span class="nav-text">7.4 Images as graphs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB"><span class="nav-text">8.图像识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E6%A6%82%E8%BF%B0"><span class="nav-text">8.1 概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="nav-text">8.2 图像分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E5%9B%BE%E5%83%8F%E6%A3%80%E6%B5%8B"><span class="nav-text">8.3 图像检测</span></a></li></ol></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">2314</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        5 posts in total
                    </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.2</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	

</main>



<script src="/js/build/libs/Swup.min.js"></script>

<script src="/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	
<script src="/js/build/tools/imageViewer.js" type="module"></script>

<script src="/js/build/utils.js" type="module"></script>

<script src="/js/build/main.js" type="module"></script>

<script src="/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/build/layouts/categoryList.js" type="module"></script>





    
<script src="/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/js/build/layouts/lazyload.js" type="module"></script>




    
<script src="/js/build/tools/runtime.js"></script>

    
<script src="/js/build/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/build/libs/Typed.min.js"></script>

  
<script src="/js/build/plugins/typed.js" type="module"></script>








    
<script src="/js/build/libs/anime.min.js"></script>





    
<script src="/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>